---
---

@misc{köpf2023openassistant,
  selected={true},
  title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, 
  author={Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
  year={2023},
  eprint={2304.07327},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = {https://arxiv.org/abs/2304.07327},
}

@inproceedings{guo-etal-2019-hierarchical,
  abbr={WMT},
  selected={true},
	title = {Hierarchical Document Encoder for Parallel Corpus Mining},
	author = {Guo, Mandy  and
		Yang, Yinfei  and
		Stevens, Keith  and
		Cer, Daniel  and
		Ge, Heming  and
		Sung, Yun-hsuan  and
		Strope, Brian  and
		Kurzweil, Ray},
	booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)},
	month = aug,
	year = {2019},
	address = {Florence, Italy},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/W19-5207},
	oi = {10.18653/v1/W19-5207},
	abstract = {We explore using multilingual document embeddings for nearest neighbor mining of parallel data. Three document-level representations are investigated: (i) document embeddings generated by simply averaging multilingual sentence embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a hierarchical multilingual document encoder (HiDE) that builds on our sentence-level model. The results show document embeddings derived from sentence-level averaging are surprisingly effective for clean datasets, but suggest models trained hierarchically at the document-level are more effective on noisy data. Analysis experiments demonstrate our hierarchical models are very robust to variations in the underlying sentence embedding quality. Using document embeddings trained with HiDE achieves the state-of-the-art on United Nations (UN) parallel document mining, 94.9\% P@1 for en-fr and 97.3\% P@1 for en-es.},
}

@inproceedings{guo-etal-2018-effective,
	abbr={WMT},
	title = {Effective Parallel Corpus Mining using Bilingual Sentence Embeddings},
	author = {Guo, Mandy  and
		Shen, Qinlan  and
		Yang, Yinfei  and
		Ge, Heming  and
		Cer, Daniel  and
		Hernandez Abrego, Gustavo  and
		Stevens, Keith  and
		Constant, Noah  and
		Sung, Yun-Hsuan  and
		Strope, Brian  and
		Kurzweil, Ray},
	booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
	month = oct,
	year = {2018},
	address = {Brussels, Belgium},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/W18-6317},
	doi = {10.18653/v1/W18-6317},
	pages = {165--176},
	abstract = {This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings. Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but have some degree of semantic similarity. The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs. We find that the sentence embeddings can be used to reconstruct the United Nations Parallel Corpus (Ziemski et al., 2016) at the sentence-level with a precision of 48.9\% for en-fr and 54.9\% for en-es. When adapted to document-level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computationally intensive approach of Uszkoreit et al. (2010). Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).},
}

@article{45610,
  abbr={ArXiv},
	title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
	author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
	year	= {2016},
	URL	= {http://arxiv.org/abs/1609.08144},
	journal	= {CoRR},
	volume	= {abs/1609.08144}
}

@inproceedings{10.5555/2390948.2391052,
  abbr={EMNLP},
  author = {Stevens, Keith and Kegelmeyer, Philip and Andrzejewski, David and Buttler, David},
  title = {Exploring Topic Coherence over Many Models and Many Topics},
  year = {2012},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  abstract = {We apply two new automated semantic evaluations to three distinct latent topic models.  Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus.},
  booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning },
  pages = {952–961},
  numpages = {10},
  location = {Jeju Island, Korea},
  series = {EMNLP-CoNLL '12}
}

@inproceedings{ccat2012,
  abbr={WordNet},
  author = {Stevens, Keith and Huang, Terry and Buttler, David},
  year = {2012},
  month = {01},
  pages = {},
  publisher = {Global Wordnet Conference 2012},
  title = {The C-Cat Wordnet Package: An Open Source Package for modifying and applying Wordnet.}
}

@inproceedings{10.5555/2140458.2140472,
  abbr={EMNLP},
  author = {Jurgens, David and Stevens, Keith},
  title = {Measuring the Impact of Sense Similarity on Word Sense Induction},
  year = {2011},
  isbn = {9781937284138},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  abstract = {Word Sense Induction (WSI) is an unsupervised learning approach to discovering the different senses of a word from its contextual uses. A core challenge to WSI approaches is distinguishing between related and possibly similar senses of a word. Current WSI evaluation techniques have yet to analyze the specific impact of similarity on accuracy. Therefore, we present a new WSI evaluation that quantifies the relationship between the relatedness of a word's senses and the ability of a WSI algorithm to distinguish between them. Furthermore, we perform an analysis on sense confusions in SemEval-2 WSI task according to sense similarity. Both analyses for a representative selection of clustering-based WSI approaches reveals that performance is most sensitive to the clustering algorithm and not the lexical features used.},
  booktitle = {Proceedings of the First Workshop on Unsupervised Learning in NLP},
  pages = {113–123},
  numpages = {11},
  location = {Edinburgh, Scotland},
  series = {EMNLP '11}
}

@article{rapiddoc2011,
  author = {Buttler, David and Andrzejewski, David and Stevens, Keith and Anastasiu, David and Gao, B},
  year = {2011},
  month = {01},
  pages = {},
  title = {Rapid Exploitation and Analysis of Documents},
  doi = {10.2172/1033748}
}

@inproceedings{jurgens-stevens-2010-capturing,
  abbr={Workshop},
  title = {Capturing Nonlinear Structure in Word Spaces through Dimensionality Reduction},
  author = {Jurgens, David  and Stevens, Keith},
  booktitle = {Proceedings of the 2010 Workshop on {GE}ometrical Models of Natural Language Semantics},
  month = jul,
  year = {2010},
  address = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/W10-2801},
}

@inproceedings{jurgens-stevens-2010-space,
  abbr={ACL},
  selected={true},
	title = {The S-Space Package: An Open Source Package for Word Space Models},
  abstract = {We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks},
	author = {Jurgens, David  and Stevens, Keith},
	booktitle = {Proceedings of the ACL 2010 System Demonstrations},
	month = {jul},
	year = {2010},
	address = {Uppsala, Sweden},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P10-4006},
}

@inproceedings{jurgens-stevens-2010-hermit,
  abbr={SemEval},
  title = {HERMIT: Flexible Clustering for the SemEval-2 WSI Task},
  author = {Jurgens, David  and Stevens, Keith},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  month = jul,
  year = {2010},
  address = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/S10-1080},
}

