@inproceedings{zhang2021lowcost,
  abbr={INSAI},
  title={Low-cost Solution for Vision-based Robotic Grasping},
  author={Zhang, Zheyuan and Shang, Huiliang},
  booktitle={International Conference on Networking Systems of AI (IEEE INSAI)},
  year={2021},
  abstract={Robotic grasping is a fundamental task for many robots to interact with the outside world, and it is still challenging. There are at least three tasks for robot grasping: object localization, grasp pose estimation, and motion planning. This paper presents a low-cost machine vision solution for robotic grasping based on template matching, including comparisons between different approaches, including state-of-the-art YOLOv4 object detection and edge-based geometric shape detection. The robotic grasping solution presented in this paper shows a high pick-and-place success rate. An improvement for template matching is implemented in this paper as well. This paper also provides detailed analysis, algorithms, and experiments.},
  selected={true},
  ieeexplore={https://ieeexplore.ieee.org/document/9757984},
  code={https://github.com/cozheyuanzhangde/Invariant-TemplateMatching}
}

@inproceedings{zhang2022ccminn, 
abbr={ONLINE},
title={A Computational Cognitive Model of Human Memory Based on Invertible Neural Networks},
author={Zhang, Zheyuan},
year={2022},
abstract={Cognitive modeling is a prerequisite for an intelligent agent to be more like humans or other animals, and memory is the basis for higher mental activities such as thinking, generating emotions, and imagining. This paper presents a computational model of memory to simulate how we remember and recall things. It demonstrates the feasibility of using neural networks to encode information stored in the memory. The computational model is based on the multi-store model of memory, which divides the memory into a sensory register, a short-term store, and a long-term store. The model accomplishes the process of recovering memory traces by using invertible neural networks (INNs). Furthermore, the paper established a bridge between artificial intelligence and psychology, in which psychology can inspire and advance AI while AI can be used to explain psychology in a computational manner.},
selected={false},
OSF={https://osf.io/58adh/}
}

@inproceedings{zhang2022botlab, 
abbr={ONLINE},
title={Bot Lab: Autonomous Ground Vehicle from Low-level Control, SLAM to Planning and Exploration},
author={Zhang, Zheyuan and Zhu, Yu and Aatitya Raajan Priyadharshini, Manu and Ashokkumar, Thirumalaesh},
year={2022},
abstract={The MBot mobile robotics project aims to develop an autonomous ground vehicle to navigate in the unknown environment. There are four key components of the project. Low-level control executes commands from high-level system to drive the robot based on velocity models and kinematics with a PID controller. Simultaneous Localization and Mapping (SLAM) is at the core of MBot project which allows the robot to use LiDAR to build a map of the environment and localize in that map at the same time. We developed mapping module, particle filter with action model and sensor model. Additionally, we implemented AStar (A*) heuristic search with pruning algorithm for path planning and frontier-guided algorithm for exploration. We present the theory and detailed imple- mentation with experimental results for the project in this report.},
selected={true},
PDF={https://cozheyuanzhangde.github.io/assets/papers/botlab.pdf}
}

@inproceedings{zhang2023har,
  abbr={EMNLP},
  title={From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning},
  author={Zhang, Zheyuan and Storks, Shane and Hu, Fengyuan and Sohn, Sungryull and Lee, Moontae and Lee, Honglak and Chai, Joyce},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  abstract={Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this challenge remains under-explored. Cognitive psychology theorizes that humans are capable of utilizing fast and intuitive heuristic thinking to make decisions based on past experience, then rationalizing the decisions through slower and deliberative analytic reasoning. We incorporate these interlinked dual processes in fine-tuning and in-context learning with PLMs, applying them to two language understanding tasks that require coherent physical commonsense reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR) strategies drastically improve the coherence of rationalizations for model decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive Physics (TRIP). We also find that this improved coherence is a direct result of more faithful attention to relevant language context in each step of reasoning. Our findings suggest that human-like reasoning strategies can effectively improve the coherence and reliability of PLM reasoning.},
  selected={true},
  arXiv={2310.18364},
  code={https://github.com/sled-group/Heuristic-Analytic-Reasoning}
}

@inproceedings{yu2023eilev,
  abbr={arXiv},
  title={Efficient In-Context Learning in Vision-Language Models for Egocentric Videos},
  author={Yu, Keunwoo Peter and Zhang, Zheyuan and Hu, Fengyuan and Chai, Joyce},
  booktitle={arXiv},
  year={2023},
  abstract={Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations. However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs. We propose a novel training method ùîºfficient ùïÄn-context ùïÉearning on ùîºgocentric ùïçideos (ùîºùïÄùïÉùîºùïç), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets. ùîºùïÄùïÉùîºùïç involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms. Our evaluations show that ùîºùïÄùïÉùîºùïç-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning. Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability.},
  selected={true},
  arXiv={2311.17041},
  website={https://yukw777.github.io/EILEV/},
  code={https://github.com/yukw777/EILEV}
}