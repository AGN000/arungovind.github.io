@article{cagnetta2022wide,
    abbr={arXiv},
    title={What can be learnt with wide convolutional neural networks?},
    author={Cagnetta*, Francesco and Favero*, Alessandro and Wyart, Matthieu},
    year={2022},
    journal={arXiv preprint arXiv:2208.01003},
    pdf={https://arxiv.org/pdf/2208.01003.pdf},
    poster={hierarchical-cnn-poster.pdf},
    abstract={Despite their success, understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the compositional and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performances, e.g. the rate of decay of the generalisation error with the number of training samples. In this paper we study deep CNNs in the kernel regime: i) we show that the spectrum of the corresponding kernel and its asymptotics inherit the hierarchical structure of the network; ii) we use generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function; iii) we illustrate this result by computing the rate of decay of the error in a teacher-student setting, where a deep CNN is trained on the output of another deep CNN with randomly-initialised parameters. We find that if the teacher function depends on certain low-dimensional subsets of the input variables, then the rate is controlled by the effective dimensionality of these subsets. Conversely, if the teacher function depends on the full set of input variables, then the error rate is inversely proportional to the input dimension. Interestingly, this implies that despite their hierarchical structure, the functions generated by deep CNNs are too rich to be efficiently learnable in high dimension.}
}

@article{favero2021locality,
  abbr={NeurIPS},
  title={Locality defeats the curse of dimensionality in convolutional teacher-student scenarios},
  author={Favero*, Alessandro and Cagnetta*, Francesco and Wyart, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  year={2021},
  repub={<em>Journal of Statistical Mechanics, Theory and Experiment</em> 2022},
  pdf={https://arxiv.org/pdf/2106.08619.pdf},
  slides={neurips21-locality-slides.pdf},
  poster={neurips21-locality-poster-web.pdf},
  video={https://nips.cc/virtual/2021/poster/27703},
  abstract={Convolutional neural networks perform a local and translationally-invariant treatment of the data: quantifying which of these two aspects is central to their success remains a challenge. We study this problem within a teacher-student framework for kernel regression, using `convolutional' kernels inspired by the neural tangent kernel of simple convolutional architectures of given filter size. Using heuristic methods from physics, we find in the ridgeless case that locality is key in determining the learning curve exponent β (that relates the decay of the test error to the size of the training set P), whereas translational invariance is not. In particular, if the filter size of the teacher t is smaller than that of the student s, β is a function of s only and does not depend on the input dimension. We confirm our predictions on β empirically. We conclude by proving, using a natural universality assumption, that performing kernel regression with a ridge that decreases with the size of the training set leads to similar learning curve exponents to those we obtain in the ridgeless case.}
}

@article{petrini2021relative,
  abbr={NeurIPS},
  title={Relative stability toward diffeomorphisms indicates performance in deep nets},
  author={Petrini, Leonardo and Favero, Alessandro and Geiger, Mario and Wyart, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  repub={<em>Journal of Statistical Mechanics, Theory and Experiment</em> 2022},
  year={2021},
  pdf={https://arxiv.org/pdf/2105.02468.pdf},
  abstract={Understanding why deep nets can classify data in large dimensions remains a challenge. It has been proposed that they do so by becoming stable to diffeomorphisms, yet existing empirical measurements support that it is often not the case. We revisit this question by defining a maximum-entropy distribution on diffeomorphisms, that allows to study typical diffeomorphisms of a given norm. We confirm that stability toward diffeomorphisms does not strongly correlate to performance on benchmark data sets of images. By contrast, we find that the stability toward diffeomorphisms relative to that of generic transformations Rf correlates remarkably with the test error ε. It is of order unity at initialization but decreases by several decades during training for state-of-the-art architectures. For CIFAR10 and 15 known architectures, we find ε≈0.2√Rf, suggesting that obtaining a small Rf is important to achieve good performance. We study how Rf depends on the size of the training set and compare it to a simple model of invariant learning.}
}
