---
---


@inproceedings{vjosa_ismir_2022,
  abbr      = {ISMIR},
  abstract  = {This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song's overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression approaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (between .20 and .30) than values of empathy and equality (between .08 and .11), while basic demographic variables only account for a small part in the models' explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.},
  title     = {More Than Words: Linking Music Preferences and Moral Values Through Lyrics},
  booktitle = {Accepted at the 23rd International Society for Music Information Retrieval Conference},
  author    = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
  year      = {2022},
  address   = {{Online}},
  selected  = {true},
  arxiv     = {2209.01169}
}

@inproceedings{charis_imrf_2022,
  abbr      = {IMRF},
  abstract  = {Our recent research has shown that people lack knowledge about how the senses interact and are unaware of many common forms of sensory and perceptual variation. We present Seeing Music, a digital interactive exhibition and audiovisual game that translates high-level scientific understanding of sensory variation and cross-modality into knowledge for the public. Using a narrative-driven gamified approach, players are tasked with communicating human music to an extraterrestrial intelligence through visual shape, color and texture using two-dimensional selector panels. Music snippets (12–24 s long) are played continuously in a loop, taken from three custom instrumental compositions designed to vary systematically in terms of timbre, melody, and rhythm. Players can “level-up” to unlock new visual features and musical snippets, and explore and evaluate collaborative visualizations made by others. Outside the game, a series of interactive slideshows help visitors learn more about sensory experience, sensory diversity, and how our senses make us human. The exhibition debuted at the 2021 Edinburgh Science Festival, where it was visited by 197 users coming from 21 countries (134 visitors from the UK) over 16 days. As it continues running online, a further 596 visitors from 35 countries (164 from the UK) have engaged. To date, 169 players of Seeing Music have produced more than 42,500 audiovisual mapping datapoints for scientific research purposes. Preliminary analysis suggests that music with less high-frequency energy was mapped to less complex and rounder shapes, bluer and less bright hues, and less dense textures. These trends confirm auditory-visual correspondences previously reported in more controlled laboratory studies, while also offering new insight into how different auditory-visual associations interact with each other. Future work includes improving user motivation and interaction, refining data collection, a full open-source release, and adding new games and informational material about research on the senses.},
  title     = {Seeing Music: Leveraging citizen science and gamification to study cross-sensory associations},
  booktitle = {20th International Multisensory Research Forum},
  author    = {Saitis, Charalampos and Cuskley, Christine and L{\"o}bbers, Sebastian},
  year      = {2022},
  address   = {{Online}},
  selected  = {false}
}

@article{hayes_disembodied_2022,
  abbr     = {JAES},
  title    = {Disembodied {{Timbres}}: {{A Study}} on {{Semantically Prompted FM Synthesis}}},
  abstract = {Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalise to electronic sounds, nor is it obvious how these relate to the creation of such sounds. In this work, we present an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. We applied a novel experimental paradigm in which experienced sound designers responded to semantic prompts by programming a synthesiser, and provided semantic ratings on the sounds they created. Exploratory factor analysis revealed a ﬁve-dimensional semantic space. The ﬁrst two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. Our results suggest that further enquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile, and that this could beneﬁt research into auditory perception and cognition, as well as synthesis control and audio engineering.},
  author   = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  year     = {2022},
  month    = may,
  journal  = {Journal of the Audio Engineering Society},
  volume   = {70},
  number   = {5},
  pages    = {373--391},
  issn     = {15494950},
  doi      = {10.17743/jaes.2022.0006},
  pdf      = {hayes_disembodied_2022.pdf},
  selected = {true}
}


@inproceedings{hayes_neural_2021,
  abbr      = {ISMIR},
  abstract  = {We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.},
  title     = {Neural Waveshaping Synthesis},
  booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference},
  author    = {Hayes, Ben and and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  year      = {2021},
  address   = {{Online}},
  selected  = {true},
  pdf       = {hayes_neural_2021.pdf}
}

@inproceedings{hayes_perceptual_2021,
  abbr      = {ICMPC},
  abstract  = {Electronic sound has a rich history, yet timbre research has typically focused on the sounds of physical instruments, while synthesised sound is often relegated to functional roles like recreating acoustic timbres. Studying the perception of synthesised sound can broaden our conception of timbre and improve musical synthesis tools. We aimed to identify the perceptually salient acoustic attributes of sounds produced by frequency modulation synthesis. We also aimed to test Zacharakis et al’s luminance-texture-mass timbre semantic model [Music Perception, 31, 339–358 (2014)] in this domain. Finally, we aimed to identify effects of prior music or synthesis experience on these results. Our results suggest that discrimination of abstract electronic timbres may rely on attributes distinct from those used with acoustic timbres. Further, the most salient attributes vary with expertise. However, the use of semantic descriptors is similar to that of acoustic instruments, and is consistent across expertise levels.},
  title     = {Perceptual and semantic scaling of FM synthesis timbres: Common dimensions and the role of expertise},
  booktitle = {16th International Conference on Music Perception and Cognition},
  author    = {Hayes, Ben and and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  year      = {2021},
  address   = {{Sheffield, UK}},
  selected  = {false},
  pdf       = {hayes_perceptual_2021.pdf}
}

@inproceedings{hayes_perceptual_2020,
  abbr      = {DMRN},
  abstract  = {Many neural audio synthesis models learn a representational space which can be used for control or exploration of the sounds generated. It is unclear what relationship exists between this space and human perception of these sounds. In this work, we compute configurational similarity metrics between an embedding space learned by a neural audio synthesis model and conventional perceptual and seman- tic timbre spaces. These spaces are computed using abstract synthesised sounds. We find significant similarities between these spaces, suggesting a shared organisational influence.},
  title     = {Perceptual {{Similarities}} in {{Neural Timbre Embeddings}}},
  booktitle = {{{DMRN}}+15: {{Digital Music Research Network One}}-Day {{Workshop}} 2020},
  author    = {Hayes, Ben and Brosnahan, Luke and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  year      = {2020},
  address   = {{London, UK}},
  selected  = {false},
  pdf       = {hayes_perceptual_2020.pdf}
}

@inproceedings{hayes_theres_2020,
  abbr      = {Timbre},
  abstract  = {Much previous research into timbre semantics (such as when an oboe is described as “hollow”) has focused on sounds produced by acoustic instruments, particularly those associated with western tonal music (Saitis & Weinzierl, 2019). Many synthesisers are capable of producing sounds outside the timbral range of physical instruments, but which are still discriminable by their timbre. Research into the perception of such sounds, therefore, may help elucidate further the mechanisms underpinning our experience of timbre in the broader sense. In this paper, we present a novel paradigm on the application of semantic descriptors to sounds produced by experienced sound designers using an FM synthesiser with a full set of controls.},
  title     = {There's More to Timbre than Musical Instruments: Semantic Dimensions of {{FM}} Sounds},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Timbre}}},
  author    = {Hayes, Ben and Saitis, Charalampos},
  year      = {2020},
  address   = {{Thessaloniki, Greece (Online)}},
  selected  = {false},
  pdf       = {hayes_theres_2020.pdf}
}

@inproceedings{zacharakis_evidence_2020,
  abbr      = {Timbre},
  abstract  = {Research on timbre perception is typically conducted under controlled laboratory conditions where every effort is made to maintain stimulus presentation conditions fixed (McAdams, 2019). This conforms with the ANSI (1973) definition of timbre suggesting that in order to judge the timbre differences between a pair of sounds the rest perceptual attributes (i.e., pitch, duration and loudness) should remain unchanged. Therefore, especially in pairwise dissimilarity studies, particular care is taken to ensure that loudness is not used by participants as a criterion for judgements by equalising it across experimental stimuli. On the other hand, conducting online experiments is an increasingly favoured practice in the music perception and cognition field as targeting relevant communities can potentially provide a large number of suitable participants with relatively little time investment from the side of the experimenters (e.g., Woods et al., 2015). However, the strict requirements for stimuli preparation and presentation prevents timbre studies from conducting online experimentation. Despite the obvious difficulties in imposing equal loudness on online experiments, the different playback equipment chain (DACs, pre-amplifiers, headphones) will also almost inevitably ‘colour’ the sonic outcome in a different way. Despite the above limitations, in a social distancing time like this, it would be of major importance to be able to lift some of the physical requirements in order to carry on conducting behavioural research on timbre perception. Therefore, this study aims to investigate the extent to which an uncontrolled online replication of a past laboratory-conducted pairwise dissimilarity task will distort the findings.},
  title     = {Evidence for Timbre Space Robustness to an Uncontrolled Online Stimulus Presentation},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Timbre}}},
  author    = {Zacharakis, Asterios and Hayes, Ben and Saitis, Charalampos and Pastiadis, Konstantinos},
  year      = {2020},
  address   = {{Thessaloniki, Greece (Online)}},
  selected  = {false},
  pdf       = {zacharakis_evidence_2020.pdf}
}

@article{2256_1_final_published,
   abbr={JASA},
   abstract = {Timbre dissimilarity of orchestral sounds is well-known to be multidimensional, with attack time and spectral centroid representing its two most robust acoustical correlates. The centroid dimension is traditionally considered as reflecting timbral brightness. However, the question of whether multiple continuous acoustical and/or categorical cues influence brightness perception has not been addressed comprehensively. A triangulation approach was used to examine the dimensionality of timbral brightness, its robustness across different psychoacoustical contexts, and relation to perception of the sounds' source-cause. Listeners compared 14 acoustic instrument sounds in three distinct tasks that collected general dissimilarity, brightness dissimilarity, and direct multi-stimulus brightness ratings. Results confirmed that brightness is a robust unitary auditory dimension, with direct ratings recovering the centroid dimension of general dissimilarity. When a two-dimensional space of brightness dissimilarity was considered, its second dimension correlated with the attack-time dimension of general dissimilarity, which was interpreted as reflecting a potential infiltration of the latter into brightness dissimilarity. Dissimilarity data were further modeled using partial least-squares regression with audio descriptors as predictors. Adding predictors derived from instrument family and the type of resonator and excitation did not improve the model fit, indicating that brightness perception is underpinned primarily by acoustical rather than source-cause cues.},
   title={Brightness perception for musical instrument sounds: Relation to timbre dissimilarity and source-cause categories},
   author={Saitis, Charalampos and Siedenburg, Kai},
   journal={The Journal of the Acoustical Society of America},
   volume={148},
   number={4},
   pages={2256--2266},
   year={2020},
   publisher={Acoustical Society of America},
   selected  = {true},
   pdf = {2256_1_final_published.pdf}
}

@incollection{Timbre_chapter1,
  abbr 	= {Chapter},
  abstract = {Timbre is a foundational aspect of hearing. The remarkable ability of humans to recognize sound sources and events (e.g., glass breaking, a friend’s voice, a tone from a piano) stems primarily from a capacity to perceive and process differences in the timbre of sounds. Roughly defined, timbre is thought of as any property other than pitch, duration, and loudness that allows two sounds to be distinguished. Current research unfolds along three main fronts: (1) principal perceptual and cognitive processes; (2) the role of timbre in human voice perception, perception through cochlear implants, music perception, sound quality, and sound design; and (3) computational acoustic modeling. Along these three scientific fronts, significant breakthroughs have been achieved during the decade prior to the production of this volume. Bringing together leading experts from around the world, this volume provides a joint forum for novel insights and the first comprehensive modern account of research topics and methods on the perception, cognition, and acoustic modeling of timbre. This chapter provides background information and a roadmap for the volume.},
  author = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen},
  booktitle = {Timbre: Acoustics, perception, and cognition},
  pages = {1--19},
  publisher = {Springer},
  title = {The present, past, and future of timbre research},
  year = {2019},
  selected  = {false},
  pdf = {Timbre_chapter1.pdf}
}

@inproceedings{IJCNN_2021_Accepted,
  abbr={IJCNN},
  abstract = {Convolutional Neural Networks have been extensively explored in the task of automatic music tagging. The problem can be approached by using either engineered time-frequency features or raw audio as input. Modulation filter bank representations that have been actively researched as a basis for timbre perception have the potential to facilitate the extraction of perceptually salient features. We explore end-to-end learned front-ends for audio representation learning, ModNet and SincModNet, that incorporate a temporal modulation processing block. The structure is effectively analogous to a modulation filter bank, where the FIR filter center frequencies are learned in a data-driven manner. The expectation is that a perceptually motivated filter bank can provide a useful representation for identifying music features. Our experimental results provide a fully visualisable and interpretable front-end temporal modulation decomposition of raw audio. We evaluate the performance of our model against the state-of-the-art of music tagging on the MagnaTagATune dataset. We analyse the impact on performance for particular tags when time-frequency bands are subsampled by the modulation filters at a progressively reduced rate. We demonstrate that modulation filtering provides promising results for music tagging and feature representation, without using extensive musical domain knowledge in the design of this frontend.},
  title={A Modulation Front-End for Music Audio Tagging},
  author={Vahidi, Cyrus and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
  booktitle={2021 International Joint Conference on Neural Networks},
  pages={1--7},
  year={2021},
  publisher={IEEE},
  selected={false},
  pdf = {IJCNN_2021_Accepted.pdf}
}

@inproceedings{Alejandro_AES151,
  abbr = {AES},
  abstract = {Vocal Percussion Transcription (VPT) aims at detecting vocal percussion sound events in a beatboxing performance and classifying them into the correct drum instrument class (kick, snare, or hi-hat). To do this in an online (real-time) setting, however, algorithms are forced to classify these events within just a few milliseconds after they are detected. The purpose of this study was to investigate which phoneme-to-instrument mappings are the most robust for online transcription purposes. We used three different evaluation criteria to base our decision upon: frequency of use of phonemes among different performers, spectral similarity to reference drum sounds, and classification separability. With these criteria applied, the recommended mappings would potentially feel natural for performers to articulate while enabling the classification algorithms to achieve the best performance possible. Given the final results, we provided a detailed discussion on which phonemes to choose given different contexts and applications.},
  title = {Phoneme Mappings for Online Vocal Percussion Transcription},
  author = {Alejandro, Delgado and Saitis, Charalampos and Sandler, Mark},
  booktitle = {Audio Engineering Society Convention 151},
  year = {2021},
  publisher = {Audio Engineering Society},
  selected = {false},
  pdf = {Alejandro_AES151.pdf}
}

@inproceedings{Lam_NIME21,
  abbr={NIME},
  abstract = {When two sounds are played at the same loudness, pitch, and duration, what sets them apart are their timbres. This study documents the design and implementation of the Timbre Explorer, a synthesizer interface based on efforts to dimensionalize this perceptual concept. The resulting prototype controls four perceptually salient dimensions of timbre in real-time: attack time, brightness, spectral flux, and spectral density. A graphical user interface supports user understanding with live visualizations of the effects of each dimension. The applications of this interface are three-fold; further perceptual timbre studies, usage as a practical shortcut for synthesizers, and educating users about the frequency domain, sound synthesis, and the concept of timbre. The project has since been expanded to a standalone version independent of a computer and a purely online web-audio version.},
  title={The Timbre Explorer: A Synthesizer Interface for Educational Purposes and Perceptual Studies},
  author={Lam, Joshua Ryan and Saitis, Charalampos},
  booktitle={The International Conference on New Interfaces for Musical Expression},
  year={2021},
  organization={PubPub},
  selected  = {false},
  pdf = {Lam_NIME21.pdf}
}

@incollection{Saitis_chap5,
  abbr = {Chapter},
  abstract = {Because humans lack a sensory vocabulary for auditory experiences, timbral qualities of sounds are often conceptualized and communicated through readily available sensory attributes from different modalities (e.g., bright, warm, sweet) but also through the use of onomatopoeic attributes (e.g., ringing, buzzing, shrill) or nonsensory attributes relating to abstract constructs (e.g., rich, complex, harsh). The analysis of the linguistic description of timbre, or timbre semantics, can be considered as one way to study its perceptual representation empirically. In the most commonly adopted approach, timbre is considered as a set of verbally defined perceptual attributes that represent the dimensions of a semantic timbre space. Previous studies have identified three salient semantic dimensions for timbre along with related acoustic properties. Comparisons with similarity-based multidimensional models confirm the strong link between perceiving timbre and talking about it. Still, the cognitive and neural mechanisms of timbre semantics remain largely unknown and underexplored, especially when one looks beyond the case of acoustic musical instruments.},
  address = {Cham},
  author = {Saitis, Charalampos and Weinzierl, Stefan},
  booktitle = {{Timbre: Acoustics, Perception, and Cognition}},
  pages = {119--149},
  publisher = {Springer},
  title = {The Semantics of Timbre},
  year = {2019},
  selected={false},
  pdf = {Saitis_chap5.pdf}
}

@inproceedings{Saitis_SMPC_22,
  abbr={SMPC},
  abstract = {Brightness is among the most studied aspects of timbre perception. Psychoacoustically, sounds described as ''bright'' vs ''dark'' typically exhibit a high vs low frequency emphasis in the spectrum. However, relatively little is known about the neurocognitive mechanisms that facilitate these “metaphors we listen with.” Do they originate in universal mental representations common to more than one sensory modality? Triangulating three different interaction paradigms, we investigated using speeded identification whether unimodal and crossmodal interference occurs when timbral brightness, as modelled by the centroid of the spectral envelope, and 1) pitch height, 2) visual brightness, 3) numerical value processing are semantically incongruent. In three online pilot tasks, 58 participants were presented a baseline stimulus (a pitch, gray square, or numeral) then asked to quickly identify a target stimulus that is higher/lower, brighter/darker, or greater/less than the baseline, respectively, after being primed with a bright or dark synthetic harmonic tone. Additionally, in the pitch and visual tasks, a deceptive same-target condition was included. Results suggest that timbral brightness modulates the perception of pitch and visual brightness, but not numerical value. Semantically incongruent pitch height-timbral brightness shifts produced significantly slower choice reaction time and higher error compared to congruent pairs; timbral brightness also had a strong biasing effect in the same-target condition (i.e., people heard the same pitch as higher when the target tone was timbrally brighter than the baseline, and vice versa with darker tones). In the visual task, incongruent pairings of gray squares and tones elicited slower choice reaction times than congruent pairings. No interference was observed in the number comparison task. We are currently following up on these results with a larger online replication sample, and an fMRI study to investigate the relevant neural mechanisms. Our findings shed light on the multisensory nature of experiencing timbre.},
  title={Auditory brightness perception investigated by unimodal and crossmodal interference},
  author={Saitis, Charalampos, and Wallmark, Zach and Liu, Annie},
  booktitle={Society for Music Perception and Cognition Conference},
  year={2022},
  selected  = {false},
  poster = {Saitis_SMPC_22.pdf}
}
