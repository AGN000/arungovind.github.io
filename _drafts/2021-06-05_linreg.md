---
layout: distill
title: Bayesian linear regression
description: with Python code
date: 2021-06-05

authors:
  - name: Vitalii Urbanevych
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    # affiliations:
    #   name: IAS, Princeton

bibliography: 2021-06-05_linreg.bib
---

**NOTE:**
Theoretical considerations are mainly taken from <d-cite key="bishop_pattern"></d-cite>.

## Model definition

Let's consider a set of observations $\mathbf{x}$ together
with observations of the target variable $t$.

Assume that our target variable $t$ is given by a deterministic function $y(\mathbf{x}, \mathbf{w})$ with additive Gaussian noise:

$$
\begin{equation}
  t = y(\mathbf{x}, \mathbf{w}) + \epsilon,
  \label{model}
\end{equation}
$$

where $\epsilon$ defines a random Gaussian noise with a zero mean and precision equal $\beta$ (inverse variance).


The function $y(\mathbf{x}, \mathbf{w})$ defines our model as **x** is a data and **w**
is a paremeters vector. Formally this function is defined as:

\begin{equation}
  y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}),
  \label{basfunc}
\end{equation}

where $\mathbf{\phi}(\mathbf{x})$ is a set of basis functions with a dummy zero component
$\phi_0(\mathbf{x}) = 1$. These functions can belong to a different families(for example Gaussian
or sigmoid functions), but in all that cases the model will be called *linear regression*
as it will be linear with respect to $\mathbf{w}$. The simplest choice of basis functions where $y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{x}$.


 So we can write

$$
\begin{equation}
  p (t \mid \mathbf{x}, \mathbf{w}, \beta) = N(t \mid \mathbf{w}^T \mathbf{\phi}(\mathbf{x}), \beta^{-1})
  \label{norm}
\end{equation}
$$

and corresponding likelihood function is


\begin{equation}
  p (\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) = \prod_{i=1}^N N(t_i \mid \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i), \beta^{-1}),
  \label{likelihood}
\end{equation}

where $\mathbf{X}$ is a vector of inputs $(\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_N)$ and $\mathbf{t}$ is a vector of target points
$\mathbf{t} = (t_1, t_2,..., t_N)$ and $\beta$ is a precision parameter.


Here we can play with some data. Let's generate 
a set of observations from the linear functions. I
generate 10 values of $x$ from the $Uniform(-1,1)$.
The taraget variable will be $t = -3x+2+N(0,1/\beta)$ with $\beta = 0.5$. 


Maximizing a log of (\ref{likelihood}) with respect to $\mathbf{w}$ one can easyli obtain fitting parameters $\mathbf{w}$ (so-called `Normal equation`)

\begin{equation}
  \mathbf{w}_{ML} = \left(\Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t},
  \label{maximized1}
\end{equation}

where $\Phi _{ij} = \phi_j(\mathbf{x}_i)$

So fited line is:

<details>
<summary>Click to see the code</summary>
  <d-code block language="python">
    import numpy as np
    import scipy
    import matplotlib.pyplot as plt

    np.random.seed(42)
    N=10
    w_orig = np.array([-3.0, 2.0]) # true values of the parameter
    x = np.random.uniform(-1, 1, N)
    beta = 2.0 # noize precision (inverse variance)
    PHI =np.vstack((x,np.ones(x.shape[0]))).T # x with additional columns with ones
    noize = np.random.normal(0.0, scale=1.0/np.sqrt(beta), size=N)
    t = PHI@w_orig + noize # target

    w_ml = (inv(PHI.T.dot(PHI))).dot(PHI.T.dot(t)) # normal equation

    plt.scatter(x, t, marker="x", label="data")
    plt.plot(x,PHI.dot(w_ml), c="tab:green", label="normal equation")
    plt.plot(x,PHI.dot(w_orig), c="tab:red", label="true line")
    plt.xlabel("x")
    plt.ylabel("t")
    plt.title(f"w = [{round(w_ml[0],2)},{round(w_ml[1],2)}]")
    plt.legend()
    plt.show()
  </d-code>

  <d-code block language="python">
    
  </d-code>

</details>


<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/linreg/fited-ml.png">
    </div>
</div>

For more complex functions one can also use iterative approach,
e.g. `Gradient descent`. In this case we set some zero approximation for the parameters and gradually come closer to true values.

<details>
<summary>Click to see the code</summary>
  <d-code block language="python">
    w0 = np.array([0.0, 0.0])[np.newaxis,:] # first guess
    eta = 0.15 # learning rate

    xc = np.linspace(-4,0.1, 100)
    yc = np.linspace(-0.1, 4, 100)

    X, Y = np.meshgrid(xc, yc)
    www = np.moveaxis(np.stack((X,Y)), 0, -1)
    Z = np.sum((t - www.dot(PHI.T))**2, axis=-1)

    for _ in range(50):
        w0 = np.vstack((w0, w0[-1] + eta*(t-w0[-1].dot(PHI.T)).dot(PHI)))
        
    plt.figure(figsize=(8,6))
    plt.contourf(X, Y, Z,50, cmap='pink')
    plt.scatter(*w_orig, facecolors='none', edgecolors='r', label="true value")
    plt.scatter(w0[1:,0], w0[1:,1], s=20)
    plt.plot(w0[:,0], w0[:,1],"--")
    plt.scatter(*w0[0], label="start", marker='x', color='red', s=80)
    plt.scatter(*w_ml, facecolors='none', c='lime',s=60, label="normal equation value")
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.legend()
    plt.savefig("../img/linreg/descent.png", dpi=500)
    plt.show()
  </d-code>

</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/linreg/descent.png">
    </div>
</div>
<div class="caption">
    Gradient descent
</div>



## Bayesian approach

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/homo-bayesians.jpg">
    </div>
</div>


In this case, we treat regression parameters not as unknown constants,
but as random variables. Therefore we shall define subjective information about
the set of parameters $\mathbf{w}$ as a prior distribution.
There are lots of possibilities but the  simplest is to 
set this distribution to be a Gaussian, 
as it is conjugate with respect to the likelihood
function \ref{likelihood}.

\begin{equation}
  p(\mathbf{w}) = N(\mathbf{w} \mid \mathbf{m_0}, \mathbf{S_0}),
  \label{prior_w}
\end{equation}
where $\mathbf{m_0}$ and $\mathbf{S_0}$ are arbitrary defined as a first approximation.

With such a simple prior one can get the corresponding posterior distribution analytically

$$
\begin{equation}
  p(\mathbf{w} \mid \mathbf{t}, \mathbf{X}, \beta) = N(\mathbf{w} \mid \mathbf{m_1}, \mathbf{S_1}),
  \label{posterior_w}
\end{equation}
$$


whith well defined parameters

$$
\begin{eqnarray}
  \mathbf{m}_1 &=& \mathbf{S}_1\left(\mathbf{S}_0^{-1}\mathbf{m_0} + \beta \mathbf{\Phi}^T\mathbf{t}\right)
  \label{m1}\\
  \mathbf{S}_1^{-1} &=& \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}.
  \label{S1}
\end{eqnarray}
$$

The (\ref{posterior_w}) is nothing else but normalized product of (\ref{likelihood}) and (\ref{prior_w}). $\mathbf{m}_1$ is a vector of means of parameters $\mathbf{w}$ and $\mathbf{S}_1$ is a covariance matrix.

Choosing $\mathbf{m_0}=(0,0)$ and $$\mathbf{S_0} = \big(\begin{smallmatrix} 10 & 0\\ 0 & 10 \end{smallmatrix}\big)$$, I obtained next results:

<details>
  <summary>Click to see the code</summary>
    <d-code block language="python">
    def likelihood(w, t, PHI, beta):
        if t.shape:
            return np.prod(scipy.stats.norm.pdf(t,loc=w.dot(PHI.T), scale=1.0/np.sqrt(beta)), axis=-1)
        else:
            return scipy.stats.norm.pdf(t,loc=w.dot(PHI.T), scale=1.0/np.sqrt(beta))
     
    def m1_S1(m0, S0, y, beta, PHI):
        if y.shape:
            S1 = np.linalg.inv(np.linalg.inv(S0) + beta*PHI.T@PHI)
            m1 = S1.dot(np.linalg.inv(S0).dot(m0) + beta*PHI.T.dot(y))
        else:
            S1 = np.linalg.inv(np.linalg.inv(S0) + beta*PHI[np.newaxis].T@PHI[np.newaxis])
            m1 = S1.dot(np.linalg.inv(S0).dot(m0) + beta*PHI.T*y)
        return m1, S1

    m0 = np.array([0,0])
    S0 = np.array([[10,0],[0,10]])
    w0 = np.linspace(-5,5,100)
    w1 = np.linspace(-5,5,100)
    w=np.array([[[val0,val1] for val0 in w0] for val1 in w1])
    pr = scipy.stats.multivariate_normal.pdf(w,mean=m0, cov=S0)

    lk = likelihood(w, t, PHI, beta)
    m1, S1 = m1_S1(m0, S0, t, beta, PHI)
    pr1 = scipy.stats.multivariate_normal.pdf(w,mean=m1, cov=S1)

    plt.figure(figsize=(15,5))
    plt.subplot(131)
    plt.contourf(w0, w1, pr, cmap='RdYlGn_r')
    plt.scatter(*w_orig, marker="+", s=100, c="b", label="true value")
    plt.scatter(*w_ml, marker='x', c='black', s=50)
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.title("prior")
    plt.subplot(132)
    plt.contourf(w0,w1,pr1, cmap='RdYlGn_r')
    plt.scatter(*w_orig, marker='x', c='b', s=50, label="True")
    plt.scatter(*w_ml, marker='x', c='black', s=50, label="Normal equation")
    plt.legend()
    plt.title("posterior")
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.subplot(133)
    plt.scatter(x, t, marker='x', label='data')
    plt.plot(x,PHI.dot(m1), c="tab:green", label="label="bayesian mean")
    plt.plot(x,PHI.dot(w_orig), c="tab:red", label="true line")
    plt.title(f"m1 = [{round(m1[0],2)},{round(m1[1],2)}]")
    plt.legend()
    plt.xlabel("x")
    plt.ylabel("t")
    plt.xlim([-1,1])
    plt.ylim([-1,5])
    plt.tight_layout()
    plt.show()
  </d-code>

</details>


<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/linreg/bayesian_fit.png">
    </div>
</div>
<div class="caption">
    Prior(left), posterior(middle) and fitted line with parameters as a mean of the posterior.
</div>

with

$$
\begin{eqnarray}
  \mathbf{m_1} &=& (-1.88,1.68)
  \label{m1_res}\\
  \mathbf{S_1}  &=& \big(\begin{smallmatrix}  1.16 & -0.14\\ -0.14 & 0.4 \end{smallmatrix}\big).
  \label{S1_res}
\end{eqnarray}
$$

The values of $\mathbf{m_1}$ are close to the values of $\mathbf{w}$ obtained within the frequentists' approach, but shifted towards $\mathbf{m_0}$ values.


## Numerical estimation of the posterior distribution ([Metropolis-Hastings sampler](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm))

Although it was easy enough to get the posterior distribution in the previous example, it is not always the case. Metropolis-Hastings method allows to sample values from complex probability functions. As posterior is proportional to the product of the likelihood and prior, it is enough to sample from such a product to find all the neccessary distribution parameters.

The first step is to test the Metropolis-Hastings sampler on the same normal prior. We know the analytical result so we can evaluate the precision of such model.

The starting point for the sampling is chosen to be $(0,0)$, the step $\delta = 0.5$ and the number of iterations is $N = 10^4$. The sampling result is below.


<details>
  <summary>Click to see the code</summary>
    <d-code block language="python">
    def posterior(w, PHI, y, beta, m0, S0):
      return likelihood(w, y, PHI, beta)*prior(w, m0, S0)

    beta=2.0
    samples = np.array([0.0, 0.0])[np.newaxis]

    # for the background colormap
    w_0 = np.linspace(-6,1,100)
    w_1 = np.linspace(-1, 5 ,100)
    w=np.array([[[val0,val1] for val0 in w_0] for val1 in w_1])
    pr = prior(w, m0, S0)
    lk = likelihood(w, data_y, data_PHI, beta) 

    # sampling
    while samples.shape[0] < N:
    candidate = samples[-1]+stats.uniform.rvs(loc=-delta, scale=2*delta, size=2)
    alpha = min(1, posterior(candidate, data_PHI, data_y, beta, m0, S0)/posterior(samples[-1], data_PHI, data_y, beta, m0, S0))
    u = stats.uniform.rvs()
    if u < alpha:
        samples = np.vstack((samples, candidate))
  </d-code>
</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/linreg/anim.gif">
    </div>
</div>
<div class="caption">
    Metropolis-Hastings sampling
</div>

Now we can find mean values of the samples and their covariance matrix.

<d-code block language="python">
  >>> print("Mean:", np.mean(samples, axis=0))
  >>> print("Covariance:\n", np.cov(samples.T))
    Mean: [-1.80861389  1.64466857]
    Covariance:
    [[ 1.04759349 -0.10366886]
    [-0.10366886  0.36495126]]
</d-code>

These values are quite close to analytical \ref{m1_res} and \ref{S1_res}. So this method sufficiently approximates posterior probability disribution.