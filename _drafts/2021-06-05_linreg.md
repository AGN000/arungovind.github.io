---
layout: distill
title: Bayesian linear regression
description: an example of a distill-style blog post and main elements
date: 2021-06-05

authors:
  - name: Vitalii Urbanevych
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    # affiliations:
    #   name: IAS, Princeton

bibliography: 2021-06-05_linreg.bib
---

**NOTE:**
This is just my interpretetion of the material from <d-cite key="bishop_pattern"></d-cite>.


## Linear regression

Assume that we have a target variable $t$:

\begin{equation}
  t = y(\mathbf{x}, \mathbf{w}) + \epsilon
  \label{model}
\end{equation}

The function $y(\mathbf{x}, \mathbf{w})$ defines our model as **x** is a data and **w**
is a paremeters vector. Formally this function is defined as:

\begin{equation}
  y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}),
  \label{basfunc}
\end{equation}

where $\mathbf{\phi}(\mathbf{x})$ is a set of basis functions with a dummy zero component
$\phi_0(\mathbf{x}) = 1$. These functions can belong to a different families(for example Gaussian
or sigmoid functions), but in all that cases the model will be called *linear regression*
as it will be linear with respect to $\mathbf{w}$. The simplest choice of basis functions where $y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{x}$.

In Eq.\ref{model} $\epsilon$ defines a random Gaussian noise with a zero mean and precision
$\beta$. So we can write

\begin{equation}
  p (t \mid \mathbf{x}, \mathbf{w}, \beta) = N(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1})
  \label{norm}
\end{equation}

and corresponding likelihood function is


\begin{equation}
  p (\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) = \prod_{i=1}^N N(t_i \mid \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i), \beta^{-1}),
  \label{likelihood}
\end{equation}

where $\mathbf{X}$ is a vector of inputs $(\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_N)$ and $\mathbf{t}$ is a vector of target points
$\mathbf{t} = (t_1, t_2,..., t_N)$.

Maximizing a log of \ref{likelihood} with respect to $\mathbf{w}$ and 
further with respect to $\beta$ will give:

\begin{equation}
  \mathbf{w}_{ML} = \left(\Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t},
  \label{maximized1}
\end{equation}

where $\Phi _{ij} = \phi_j(\mathbf{x}_i)$

\begin{equation}
  \frac{1}{\beta _{ML}} = \frac{1}{N} \sum _{i=1}^N \left( t_i - \mathbf{w} _{ML}^T \phi(\mathbf{x}_i) \right)^2
  \label{maximized2}
\end{equation}



## Bayesian approach

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align='center'>
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/homo-bayesians.jpg">
    </div>
</div>

In this case, we shall define subjective information about
the set of parameters $\mathbf{w}$ as a prior distribution.
There are lots of possibilities but the  simplest is to 
set this distribution to be a Gaussian, 
as it is conjugate with respect to the likelihood
function \ref{likelihood}.

\begin{equation}
  p(\mathbf{w}) = N(\mathbf{w} \mid \mathbf{m_0}, \mathbf{S_0})
  \label{prior_w}
\end{equation}


The corresponding posterior distribution will be:

\begin{equation}
  p(\mathbf{w}) = N(\mathbf{w} \mid \mathbf{m_1}, \mathbf{S_1}),
  \label{posterior_w}
\end{equation}

where

$$
\begin{eqnarray}
  \mathbf{m}_1 &=& \mathbf{S}_1\left(\mathbf{S}_0^{-1}\mathbf{m_0} + \beta \mathbf{\Phi}^T\mathbf{t}\right)
  \label{m1}\\
  \mathbf{S}_1^{-1} &=& \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}
  \label{S1}
\end{eqnarray}
$$

### Marcov Chain Monte Carlo estimation of \ref{posterior_w} (Metropolis-Hasting or Gibbs sampler)