<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }

  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114291442-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-6');
</script>


<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="../img/favicon.ico">
  <title>Navigating to Objects in the Real World</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://www.youtube.com/iframe_api"></script>
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Navigating to Objects in Real-world</span></center><br/>
      <table align=center width=600px>
      <tr>
        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://theophilegervet.github.io/" target="_blank">Theo Gervet</a></span></center></td>

        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://soumith.ch/" target="_blank">Soumith Chintala</a></span></center></td>
          
        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://faculty.cc.gatech.edu/~dbatra/" target="_blank">Dhruv Batra</a></span></center></td>

        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a></span></center></td>
          
        <td align=center width=160px>
        <center><span style="font-size:22px"><a href="https://devendrachaplot.github.io/" target="_blank">Devendra Singh Chaplot</a></span></center></td>
          

    

      <tr/>
      <tr>
        <td align=center width=100px>
        <center><span style="font-size:20px">CMU</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
          <center><span style="font-size:20px">FAIR</span></center></td>
      <tr/>
      </table>
      <!-- <table align=center width=700px> -->
          <!-- <tr> -->
            <!-- <td align=center width=700px><center><span style="font-size:22px">Published at <a href="https://neurips.cc/">NeurIPS, 2021</a> </span></center></td> -->
          <!-- <tr/> -->
      <!-- </table><br/> -->
      <table align=center width=700px>
          <tr>
            <td align=center width=100px><center><span style="font-size:28px"><a href="">[Paper]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="">[Code]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="">[Talk]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="">[Slides]</a></span></center></td>
<!--            <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/devendrachaplot/Neural-SLAM'>[GitHub Code]</a></span></center></td>-->
          <tr/>
      </table><br/>

<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
<!--      <iframe width="768" height="432" src="https://youtu.be/tlyz68j_jvE" frameborder="0" allowfullscreen></iframe>-->
<!--          <iframe width="768" height="432" src="https://www.youtube.com/embed/tlyz68j_jvE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
          <center><img src = "./resources/seal.gif" height="300px"></img></a><br></center>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        Semantic navigation is necessary to deploy mobile robots in uncontrolled environments like our homes, schools, and hospitals. 
        Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation, which builds a geometric map using depth sensors and plans to reach point goals. 
        Broadly, end-to-end learning approaches reactively map sensor inputs to actions with deep neural networks, while modular learning approaches enrich the classical pipeline with learning-based semantic sensing and exploration. 
        While the field has no shortage of proposed methods, learned navigation policies have predominantly been evaluated in simulation, and it is unclear whether simulation is useful as an evaluation benchmark. 
        We address this issue through a large-scale empirical evaluation comparing representative methods from classical, modular learning, and end-to-end learning approaches across six visually diverse homes. 
        We find that modular learning transfers well, rising from a 81% simulation to a 90% real-world success rate. 
        In contrast, end-to-end learning does not, dropping from 77% simulation to 23% real-world success rate due to a large image domain gap between simulation and reality. 
        For practitioners, we show that modular learning is a reliable approach to navigate to objects. 
        For researchers, we identify two key issues that prevent today's simulators from being reliable evaluation benchmarks: (A) a large Sim-to-Real gap in images, which causes design choices to easily overfit to simulation, and (B) a disconnect between simulation and real-world error modes, which limits the usefulness of simulation to diagnose bottlenecks and further improve methods.
      </div>
      <br><hr>

      <center><h1>SEAL: Self-Supervised Embodied Active Learning</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        Our framework called Self-supervised Embodied Active Learning (SEAL) consists of two phases, <b>Action</b>, where we learn an active exploration policy, and <b>Perception</b>, where we train the Perception Model on data gathered using the exploration policy and labels obtained using spatio-temporal label propagation. Both action and perception are learnt in a completely self-supervised manner without requiring access to the ground-truth semantic annotations or map information.
      </div><br/>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="resources/overview_seal.png"><img src = "resources/overview_seal.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>3D Semantic Mapping</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
      The 3D Semantic Mapping module takes in a sequence of RGB and Depth images and produces a 3D Semantic Map.
      </div><br/>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="resources/seal_3d_semantic_map.png"><img src = "resources/seal_3d_semantic_map.png" width="600px"></img></a><br></center>
        </td></tr>
      </table>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="resources/seal_3dmap.gif"><img src = "resources/seal_3dmap.gif" width="600px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Learning Action</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
      We define an intrinsic motivation reward called <b>Gainful Curiosity</b> to train the active exploration policy to learn such behavior of maximizing exploration of objects with high confidence. We define <em>s'</em> (= 0.9) to be the score threshold for confident predictions. The Gainful Curiosity reward is then defined to be the number of voxels in the 3D Semantic Map having greater than <em>s'</em> score for at least one semantic category. This reward encourages the agent to find new objects and keep looking at the object from different viewpoints until it gets a highly confident prediction for the object from at least one viewpoint.
      </div><br/>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="resources/seal_gainful_curiosity.gif"><img src = "resources/seal_gainful_curiosity.gif" width="600px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Learning Perception</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
      We present a method called <b>3DLabelProp</b> to obtain self-supervised labels from the 3D Semantic map. To perform disambiguation, each voxel is labelled with the category having the maximum score above <em>s'</em>. If all categories have a score less than <em>s'</em> for a voxel, we label it as not belonging to any object category. After labeling each voxel in the map, we find the set of connected voxels labeled with the same category to find object instances. The instance label for each pixel in each observation in the trajectory is then obtained using ray-tracing in the labeled 3D map based on the agent's pose. Pixel-wise instance labels are used to obtain masks and bounding boxes for each instance. Note that this labeling process is completely self-supervised and does not require any human annotation. The set of observations and self-supervised labels are used to fine-tune the pre-trained perception model.
      </div><br/>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="resources/seal_labelprop.gif"><img src = "resources/seal_labelprop.gif" width="600px"></img></a><br></center>
        </td></tr>
      </table>
      
      <br/><hr>

      <center><h1>Short Presentation</h1></center>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/NmObuQRLNWA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=200px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="../papers/neurips21_seal.pdf"><img style="width:200px" src="resources/thumbnail_nts.jpeg"/></a>
        <center>
        <span style="font-size:20pt"><a href="../papers/neurips21_seal.pdf">[Paper]</a>
<!--        <span style="font-size:20pt"><a href="https://arxiv.org/abs/1902.05546v2">[ArXiv]</a>-->
<!--        <span style="font-size:20pt"><a href="resources/slides.pdf">[Slides]</a></span>-->
<!--        <span style="font-size:20pt"><a href="resources/poster.pdf">[Poster]</a></span>-->
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
<!--            Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A. and Salakhutdinov, R., 2020. Learning To Explore Using Active Neural SLAM. In International Conference on Learning Representations (ICLR).-->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Chaplot, D.S., Dalal, M., Gupta, S., Malik, J. and Salakhutdinov, R. 2021. SEAL: Self-supervised Embodied Active Learning. In NeurIPS.</span></p>
        <!-- <p style="margin-top:20px;"></p> -->
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span>
        <div class="paper" id="assemblies19_bib">
                <pre xml:space="preserve">
@inproceedings{chaplot2021seal,
  title={SEAL: Self-supervised Embodied Active Learning},
  author={Chaplot, Devendra Singh and Dalal, Murtaza, and Gupta, Saurabh 
          and Malik, Jitendra and Salakhutdinov, Ruslan},
  booktitle={NeurIPS},
  year={2021}}
                </pre>
          </div>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          
          </td>
          </tr>
      </table>
    <br><hr>

    <center><h1>Related Projects</h1></center>
      <table align=center width=900px>
      <tr>
          <td align=center width=300px>
              <a href="./neural-slam.html">Active Neural SLAM</a>
          <center><a href="./neural-slam.html"><img src = "../img/pubs/ans.gif" height="120px"></img></a><br></center>
          </td>
          <td align=center width=300px>
              <a href="./semantic-exploration.html">Semantic Exploration</a>
          <center><a href="./semantic-exploration.html"><img src = "../img/pubs/semexp.gif" height="120px"></img></a><br></center>
          </td>
          <td align=center width=300px>
              <a href="./SemanticCuriosity.html">Semantic Curiosity</a>
          <center><a href="./SemanticCuriosity.html"><img src = "../img/pubs/semcur.gif" height="120px"></img></a><br></center>
          </td>
      </tr>
      </table>
    <br><hr>

    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      Carnegie Mellon University effort was supported in part by the US Army Grant W911NF1920104 and DSTA. UIUC effort is partially funded by NASA Grant 80NSSC21K1030. Jitendra Malik received funding support from ONR MURI (N00014-14-1-0671). Ruslan Salakhutdinov would also like to acknowledge NVIDIA’s GPU support.       
    <br>
          Website template from <a href="https://richzhang.github.io/colorization">here</a> and <a href="https://pathak22.github.io/modular-assemblies/">here</a>. <br>
      </left></td></tr>
    </table>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>