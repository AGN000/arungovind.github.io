---
layout: page
title: bio
permalink: /bio
description:
nav: true
---

I am currently a **Researcher** in the Cognitive Computing Lab at [Baidu Research](http://research.baidu.com/) working with [Dr. Ping Li](http://research.baidu.com/People/index-view?id=111) on generative modeling and its applications in Information Retrieval and AI Security. I am also a member of [Prof. Chandan K. Reddy](https://people.cs.vt.edu/reddy)'s lab at VT and the [Sanghani Center for Artificial Intelligence & Data Analytics](https://sanghani.cs.vt.edu/) since 2016. From May 2019 to Feb 2020, I was at [Criteo AI Lab](https://ailab.criteo.com/) in Palo Alto, CA, where I worked with [Dr. Sathiya Keerthi Selvaraj](http://www.keerthis.com/) and Dr. Fengjiao Wang. Before that, I was a Faculty Research Associate of [Earth System Science Interdisciplinary Center](http://essic.umd.edu/) at [UMD](https://www.umd.edu/) and also had a joint appointment at [NASA Goddard Space Flight Center](https://www.nasa.gov/goddard), where I worked on high-performance and distributed system research. I received my Ph.D. in Computer Science from [Virginia Polytechnic Institute and State University](cs.vt.edu), and MS in Computer Science from [University of Maryland, College Park](cs.umd.edu).

<h1 class="post-title">{{ "Research Interests"}}</h1><a name="research_interests"></a>
My research focuses on understanding the practical limits of using existing ML models in the real-world. Essential, I seek answers to the following question: *Are complex models easier-to-use and reliable for practical uses*?
**Easy-to-use** refers to the ability to build the model easily, execute the deployed model efficiently, and evolve the deployed model with less effort. **Reliable** relates to whether we can rely on the model to solve the intended task well, whether its performance is preserved under perturbed environments in practice (e.g., data corruption or distributional changes), and whether the model is resilient to various forms of security attacks (e.g., adversarial examples and causal attacks). In this sense, I believe that many existing ML models/methods, as *complex* as neural networks, are *reliable* but *not easy-to-use* because they do not satistify various practical assumptions.

My goal, therefore, is to develop computational frameworks that enable existing complex/deep models to be more suitable for practical uses. I focus on improving the following aspects of existing models: (i) **training**, (i) **inference**, (iii) **realistic assumptions**, and (iv) **security understanding**. Most of my ML/AI solutions center around generative-based approaches that have low computational complexity and require less human effort.

<b>{{ "Information Retrieval and Applications"}}</b>

* Interpretable Graph Similarity Computation via Differentiable Optimal Alignment of Node Embeddings (*SIGIR* 2021 by Doan et al.)
* Efficient Implicit Unsupervised Text Hashing using Adversarial Autoencoder (*WWW* 2020 by Doan et al.)
* Image Hashing by Minimizing Discrete Component-wise Wasserstein Distance (*arxiv* 2021 by Doan et al.)
* Generative Hashing Network (*Under submission* 2021 by Doan et al.)
* EBM Hashing Network (*Under submission* 2021 by Doan et al.)
* Fast Learning-to-Hash Ranking  (*Under submission* 2021 by Doan et al.)
* One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching  (*CVPR* 2022 by Doan et al.)

<b>{{ "Generative Models" }}</b>

* Image Generation Via Minimizing Frechet Distance in Discriminator Feature Space (*Under submission* 2021 by Doan et al.)
* Regression via implicit models and optimal transport cost minimization (*arxiv* 2020 by Manchanda et al.)

<b>{{ "AI Backdoor Security with Generative Models" }}</b>

* Attack with Stealthy Embedding Space Modification (*Under submission* 2021 by Doan et al.)
* Imperceptible Backdoor Attacks (*ICCV* 2021 by Doan et al.)
* Backdoor Attacks for Vision Transformers   (*Under Submission* 2022 by Doan et al.)
* Adversarial Defenses for Vision Transformers   (*Under Submission* 2022 by Peng et al.)
* Backdoor Attacks with Any Target (*Under Submission* 2022 by Doan et al.)

<h1 class="post-title">{{ "Professional Service"}}</h1><a name="services"></a>

<b>{{ "Program Committee (Invited):" }}</b>

* International Conference on Learning Representations (**ICLR**): 2021-2022
* Annual Conference on Neural Information Processing Systems (**NeurIPS**): 2020-2022
* IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**): 2020-2022
* International Conference on Machine Learning (**ICML**): 2020-2022
* IEEE International Conference on Computer Vision (**ICCV**): 2021
* European Conference on Computer Vision (**ECCV**): 2020-2022
* AAAI Conference on Artificial Intelligence (**AAAI**): 2021-2022
* IEEE International Conference on Big Data (**BigData**): 2020-2022
* 1st International Workshop on Industrial Recommendation Systems (**IRS**): 2020-2021

<b>{{ "Conference Reviewer:" }}</b>

* ACM SIGKDD International Conference on Knowledge discovery and data mining (**KDD**): 2017-2019
* ACM InternationalConferenceonInformationandKnowledgeManagement(**CIKM**):2017- 2019
* ACM International Conference on Web Search and Data Mining (**WSDM**): 2017-2019
* The Web Conference (WWW): 2017-2019
* International Joint Conference on Artificial Intelligence (**IJCAI**): 2017-2019

<b>{{ "Journal Reviewer:" }}</b>

* ACM Transactions on Knowledge Discovery from Data (**TKDD**): 2020
* ACM Transactions on Internet Technology (**TOIT**): 2018-2021
