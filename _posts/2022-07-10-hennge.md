---
layout: distill
title: Configuring Python Code
description: Best Practices To Configure Python Projects
date: 2022-08-01
disqus_comments: true


authors:
  - name: Shukrullo Nazirjonov
    affiliations:
      name: Nagoya University, Japan


bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Python Packaging
  - name: Relative Imports
  - name: Useful trick
  - name: Stepdown Rule
  - name: Recap

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

Python is an amazing language. It enables me to quickly prototype a solution to any kind of scripting tasks.
However, as my code grows in size and complexity, I find myself refactoring stuff around, creating extra tests that I should have created a week ago, and creating config files for seemingly simple stuff. 
I do not have the vision for the perfect organization of my project; I guess most people don't and that's okay.  

Only way to efficiently organize code is by making mistakes and being willing to change mindset.
These are some of the useful tricks I learned over the past couple of weeks. This writing is merely a reminder for my future self. If you find it useful, I am glad!

## Python Packaging
Python Packaging seems quite straightforward at the first glance. It usually requires a setup.py file to identify the modules in the package and record some meta-data about the package. Here's a minimal code for setup.py file:
{% highlight python %}
from setuptools import setup
setup(
     name='yourproject',
     version='0.0.1',
     description='Shrek's first repo on Github',
     url='https://github.com/shrekshreky/shrek',
     author='Shrek Shreksky',
     author_email='shrekshreksky2000@gmail.com',
     licence='MIT',
     packages=['src','mypkg/dataio','mypkg/helpers'],
)
{% endhighlight %}

The most important thing here is to list all the packages, subpackages, sub-subpackages in your project. For smaller projects, it is ok to list packages manually. However, if your project is more complex with many subpackages, then **setuptools.find_packages()** helps you locate them automatically. Use the **include** keyword argument to find only the given packages.

{% highlight python lineos %}
packages=find_packages(include=['sample','sample.*']),
{% endhighlight %}

You can add all the extra details into this setup.py file, of course, but this is the minimal setup file that I found to be applicable in many cases. Read more about python packaging in this [post](https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/#setup-args)

## Relative Imports
If you have multiple files in the same directory and you want to import a function from one to another, it is quite simple:
{% highlight bash %}
mypkg/
   first.py
   second.py
   third.py
{% endhighlight %}
And you can call **from first import func1** in _second.py_ and **from second import func2** in _third.py_ similarly.
However,let's say your scripts are in different folders in the following manner and you want to import from first.py and second.py. 
{% highlight bash %}
mypkg/
   subpkg/
     first.py
     second.py
    third.py
{% endhighlight %}
You have to add an **__init__.py** inside subpkg folder so that it can be identified. **__init__.py** marks directories as Python package directories. The common practice is to leave them empty. For our example, you can configure the **__init__.py** like this:
{% highlight bash %}
__all__=['first','second']
{% endhighlight %}
Or like this:
{% highlight bash %}
from .first import *
from .second import *
{% endhighlight %}
Also say we have a single module ( **helpers.py**) that contains implementation of **PlotGraphs** and **Transforms** classes. Over time, that single module grew into a package with modules for **plot** and **transforms**.
To ensure consistent API, it is best to add this into __init__.py in the helpers package:
{% highlight python %} 
from .plot import PlotGraphs
from .transforms import Transforms
{% endhighlight %}
This makes sure the existing user will import via {% highlight python %} from helpers import PlotGraphs {% endhighlight %} instead of {% highlight python %} from helpers.plot import PlotGraphs{% endhighlight %}

This can be quite useful in many cases and it also makes sure that PlotGraphs and Transforms are the only parts of the module intended for public use ( explicit vs implicit).
## Useful Trick
Sometimes you want to access package absolute path and make sure your code runs the same in other machines. __file__ is a variable that contains the path to the moule that is currently being imported. Using **__file__** combined with _os.path_ module lets all paths be relative to the current module's directory location which allows the package to be portable to other machines. Consider this code structure:
{% highlight bash %}
   mypkg/
     scripts/
       first.py
     models/
       RNN.py
       train.py
     dataset/
       annotate.csv
     src/
       helpers/
{% endhighlight %}
In **train.py** you want to import a dataset __annotate.csv__ from dataset folder.
One way to accomplish it is by defining a **base_path** inside train.py in the following manner:
{% highlight python %} 
base_path= os.path.abspath(os.path.dirname(__file__))
df=pd.read_csv(os.path.join(base_path,"/annotate.csv"))
{% endhighlight %}
Or consider if you want to make sure some folders gets created automatically at the right place when you run **first.py**. Again, you can define **base_path** inside **first.py** like this:
{% highlight bash %}
base_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__))) # /home/usr/mypkg
dirs=[]
dirs.append(f"{base_path}/datasets/")
dirs.append(f"{base_path}/models/")
for dir in dirs: os.makedirs(dir,exist_ok=True)
{% endhighlight %}
This way when you run the code in different machines, as long as you have the same code structure, it will run smoothly.
Honestly, I do not know if there is a better way to achieve the same goal in a more simpler and different way. ( Let me know if there are)
## Stepdown Rule
I read about this rule called " stepdown rule " somewhere, probably in a book or blogpost but it kind of got stuck in my head. Basically, it aims to make the code readable by listing abstractions in a top-down narrative. We want every function to be followed by those at the next level of abstraction so that we can read the program, descending one level of abstraction at a time. Consider this:
{% highlight python %} 
   To do A we do B and then C.
   To do B, if E we do F and otherwise we do G 
   To determine if E, we …
   To do F we …
   To do G we...
   To do B we…
   To do C we…
{% endhighlight %}

## Recap
Of course, take everything I have written above with a grain of salt. I myself trying to figure out an optimal way to organize Python Code and my methods may not be the most efficient. I will hopefully update these soon.
