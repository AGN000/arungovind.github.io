---
layout: distill
title: Variational Autoencoder explanation
description: Math and ideas behind VAE.
date: 2022-08-25
tags: ["Generative Model", "VAE"]

authors:
  - name: Giang Vu Long
    affiliations:
      name: Hanoi

bibliography: vae.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Likelihood-based generative models
  - name: Latent variable models
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Prior and importance sampling
  - name: Another perspective
  - name: Conclusion
    subsections:
    - name: Props
    - name: Cons
    - name: Further works

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Likelihood-based generative models

Generative Model is one of two type of deep learning models, which generates data by sampling from the distribution of training data. That process requires the model to understand or have the ability to simulate the distribution of the given data. To do that, one straight forward approach is modeling the density function of data by a neural network $$p_{\theta}(x)$$. That class of models called likelihood-based generative models. The objective is to maximize the likelihood function indexed by a set of paramters $$\theta$$:

$$
\begin{align*}
  \max_{\theta} \sum_{i} \log p_{\theta}(x^(i))
\end{align*}
$$

The problem, from here, is to choose a proper architecture that can not only **efficiently calculate the likelihood** $$p_{\theta}(x)$$ for training but also **easily sample** from. There are multiple ways to achieve that, such as autoregressive models, using an assumption of time-series data, or flow model. In this blog, we explore another approach which does not calculate exactly the likelihood but approximates it by an inferencing technique known as variational inference. 

***

## Latent variable models

VAE is inspired by latent variable models, which lies on an assumption of data - there is a compact representation of a data point in a smaller space. The representation is known as latent code or latent representation. That means, again, one can encode a data point in the original space into a code in the latent space (look at the below image) while maintaining the properties of data distribution and easily reversing from the latent code.

$$
  p_{\theta}(x) = \mathbb{E}_{z \sim p_{Z}} p_{\theta}(x\|z)
$$

Technically, in case of descrete latent variable $$z$$, $$p_{\theta}(x)$$ can be transformed to $$\sum_{z}p_{Z}(z)p_{\theta}(x\|z)$$. Unforturnately, in many real-world problems, $$z$$ is continuous. Then, the challenge is calculating $$\int_{z}p_{Z}(z)p_{\theta}(x\|z)dz$$. In that case, likelihood term $$p(x)$$ can not be exactly calculated but approximated by some techniques. In next section, we will explore one of that techniques called variational inference.

***

## Prior and importance sampling

The straight forward one to approximate $$\int_{z}p_{Z}(z)p_{\theta}(x\|z)dz$$ is to sample from $$z$$ space:

$$
\begin{align*}
  \log p_{\theta}(x) &= \log \int_{z}p_{Z}(z)p_{\theta}(x\|z)dz \\
  &\approx \log \frac{1}{K} \sum_{k} p_{\theta}(x\|z_{k})
\end{align*}
$$

The problem is that sampling $$K$$ samples under $$p_{Z}(z)$$ is hard because that distribution is unknown. That leads to the ideas of variational inference.

$$
\begin{align*}
\mathbb{E}_{p_{Z}(z)} p_{\theta}(x\|z) &= \int_{z}p_{Z}(z)p_{\theta}(x\|z)dz \\
&= \int_{z}\frac{q(z)}{q(z)}p_{Z}(z)p_{\theta}(x\|z)dz \\
&= \mathbb{E}_{q(z)}\frac{p_{Z}(z)}{q(z)}p_{\theta}(x\|z) \\  
&\approx \frac{1}{K}\sum_{i=1}^{K} \frac{p_{Z}(z)}{q(z)}p_{\theta}(x\|z) 
\end{align*}
$$

$$q(z)$$ is called prior distribution and is a predefined distribution. That mean, we can sample $$K$$ samples from it easily.

***

## Another perspective

***

## Conclusion

1. Props:
  
2. Cons:

3. Further work:

