---
layout: distill
title: Variational Autoencoder explanation
description: Math and ideas behind VAE.
date: 2022-08-25
tags: ["Generative Model", "VAE"]

authors:
  - name: Giang Vu Long
    affiliations:
      name: Hanoi

bibliography: vae.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Likelihood-based generative models
  - name: Latent variable models
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Prior and importance sampling
  - name: Another perspective
  - name: Conclusion
    subsections:
    - name: Props
    - name: Cons
    - name: Further works

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Likelihood-based generative models

Generative Model is one of two type of deep learning models, which generates data by sampling from the distribution of training data. That process requires the model to understand or have the ability to simulate the distribution of the given data. To do that, one straight forward approach is modeling the probilistic density function of data by a neural network $$p_{\theta}(x)$$. The problem, from here, is to choose a proper architecture that can not only **efficiently calculate the likelihood** $$p_{\theta}(x)$$ for training but also **easily sample** from. There are multiple ways to achieve that, such as autoregressive models, using an assumption of time-series data, or flow model. In this blog, we explore another approach which does not calculate exactly the likelihood but approximates it by an inferencing technique known as variational inference. 

***

## Latent variable models

VAE is inspired by latent variable models, which lies on a assumption of data - there is a compact representation of a data point in a smaller space. The representation is known as latent code or latent representation. That means, again, one can encode a data point in the original space into a data point in the latent space (look at the below image) while maintaining the properties of data distribution and easily reversing from the latent code.

Technically, the likelihood $$p_{\theta}(x)$$ can be transformed to $$\sum_{z}p(z)p_{\theta}(x\|z)$$. The mentioned value can be easily calculated when $$z$$ is a discrete variable. Unforturnately, in many real-world problems, $$z$$ is continuous. Then, the challenge is tricky and resource-consuming calculation of $$\sum_{z}p(z)p_{\theta}(x\|z)$$.

***

## Prior and importance sampling

***

## Another perspective

***

## Conclusion

1. Props:
  
2. Cons:

3. Further work:

