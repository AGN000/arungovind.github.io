---
layout: post
title: a disentanglement metric for tabular data 
date: 2023-05-10 11:59:00-0400
description: an approach to quantifying disjointedness of learned latent representations 
tags: math code
giscus_comments: true
related_posts: false
---

## Motivation 
Disentanglement in generative modelling is the idea of decomposing the underlying abstract factors driving data expression into semantically seperable units. A universally accepted definition of this idea coming from Bengio is essentially that in a disentangled paradigm if you change a single generative factor in the data you only influence the expression of a single latent variable.  For GANs or autoencoder-based architectures focused on image generation this could look something like "by forcing the embedding for my sample at position $$i$$ to take value $$z_{i}$$, I can control which direction the person in my reconstructed image is facing". So essentially for image data, disentanglement lets you align abstract factors of variation which are intuitive for humans (face shape, hair colour, azimuth) with explicitly enumerated latent dimensions. 

A few definitions and methods have come up over the years for assessing the degree of disentanglement a given model achieves, but at least from what I've seen they more so focus on non-tabular data like images. In the case where each feature in your sample represents a significant attribute, however, independence in your latent space could be interpreted as each latent unit controlling the expression of a subset of well-defined features. 

**pic showing the diagram of input, embedding, output with colours over subsets** 

***

## Deriving the metric 
Current disentanglement metrics like the one proposed by [Higgens et al.](https://openreview.net/forum?id=Sy2fzU9gl) and [Kim & Mnih](https://arxiv.org/abs/1802.05983) try to quantify disentanglement at the latent scale; that is they map inputs to embeddings and derive metrics which rely on labelled generative factors in the data to quantify independence. In contrast, the one I'm considering relies instead on the reconstructions and the idea that for complete disentanglement each latent dimension should control the expression of a unique collection of features. 

Suppose you have a dataset described by a feature space 
$$\boldsymbol{\mathcal{F}}\subset \mathbb{R}^{m}\times \mathbb{Z}^{m'}$$, 
where each coordinate captures a unique descriptor of the sample.  We're interested in establishing a correspondence between the individual units of the low dimensional embedding space, 
$$\mathcal{Z} \subset \mathbb{R}^{n}$$, $$n<<m+m'$$,
generated by a given model and a partition of $$\boldsymbol{\mathcal{F}}$$ given by 
$$\mathcal{P} = \{\mathcal{F}_{1}, \mathcal{F}_{2}, ..., \mathcal{F}_{n}\}$$ 
such that $$|\mathcal{P}| = n$$.  In this way you can map each $$z_{i}\mapsto \mathcal{F}_{i}$$ without loss of generality. For simplicity assume the embeddings are produced by some encoder, and that the reconstructions come from a corresponding decoder. With this structure in place you have a mechanism for assessing which features are sensitive to perturbations in your latent variables by "tweaking" an embedding coordinate and checking how your reconstruction changes. 


*** 

## Implementation 

```python
from torch import nn 
import torch.distributions as dist 
import numpy as np 
```