{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3ae230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing...............\n",
      "Time taken to preprocess features in seconds: 0.08747246209532022\n",
      "Shape of the dataframe: (144, 29)\n",
      "Ending reading Payload!\n",
      "Output files available: ['-2022-07-18-08-30-05/output/output.tar.gz', '-2022-07-18-14-21-07/output/output.tar.gz', '-2022-07-19-04-57-41/output/output.tar.gz', '-2022-07-19-05-51-46/output/output.tar.gz', '-2022-07-25-09-01-53/output/output.tar.gz', '-2022-07-25-09-10-03/output/output.tar.gz', '-shift-242022-07-22-03-38-01/output/output.tar.gz']\n",
      "File chosen: intraday/2022/07/lgbm-intraday-shift-242022-07-22-03-38-01/output/output.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking endpoint with payload data\n",
      "harry/year=2022/month=7/day=25/\n",
      "File Exits\n",
      "harry/year=2022/month=7/day=26/\n",
      "File Exits\n",
      "index =  66\n",
      "66 -106.19780157339841 \n",
      "\n",
      "67 -191.9337508347731 \n",
      "\n",
      "68 -22.493092258727998 \n",
      "\n",
      "69 5.185060368769268 \n",
      "\n",
      "70 65.51090507823932 \n",
      "\n",
      "71 61.371475400900636 \n",
      "\n",
      "72 -52.72628621725866 \n",
      "\n",
      "73 19.91677108200124 \n",
      "\n",
      "-120.2 2022-07-25 09:00:00  \n",
      "\n",
      "Lasted NIV Index in Result file :  66 , at  2022-07-25 09:00:00\n",
      "Successfull\n"
     ]
    }
   ],
   "source": [
    "########## Version 1.0 ################# till 20/07\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "import joblib\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "#         basename_template = f\"{file_name}.parquet\"\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + dt.timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_niv(dframe):\n",
    "    niv_df = dframe[[\"local_datetime\", \"ImbalanceQuantity(MAW)(B1780)\"]].dropna(how='any').iloc[-1:]  \n",
    "    niv_val = niv_df.iloc[0]['ImbalanceQuantity(MAW)(B1780)']\n",
    "    niv_time = niv_df.iloc[0]['local_datetime']\n",
    "    return niv_val, niv_time\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "def parse_datetime_to_str(dt_obj):\n",
    "    \"\"\"Parses a datetime object to string for model callbacks\"\"\"\n",
    "    return (dt_obj\n",
    "            .strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "            .replace(\"-\", \"\")\n",
    "            .replace(\" \", \"_\"))\n",
    "\n",
    "\n",
    "def read_to_dataframe(file_path, file_format=\"csv\", **kwargs):\n",
    "    \"\"\"Read a file to DataFrame in memory\"\"\"\n",
    "    assert file_format in [\"csv\", \"parquet\"], \\\n",
    "        \"Only .csv and .parquet files are supported\"\n",
    "    # read data into memory, assumes no column is in datetime format requiring parsing\n",
    "    if file_format == \"csv\":\n",
    "        data = pd.read_csv(file_path, **kwargs)  # TODO deal with __index__\n",
    "    elif file_format == \"parquet\":\n",
    "        data = pd.read_parquet(file_path, **kwargs)\n",
    "    return data\n",
    "\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # parse weekday information\n",
    "    dataframe[\"weekday\"] = dataframe[\"SettlementDate\"].apply(lambda x: x.isoweekday())\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"month\"] * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "    dataframe.drop([\"local_datetime\",\n",
    "        \"month\",  # information is already encoded\n",
    "        \"SettlementPeriod\",  # information is present in hour already, so remove\n",
    "        \"year\",\n",
    "        \"weekday\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_and_clip_outliers(dataframe, cutoff=3):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) / col_std > cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                z_cutoff = cutoff * col_std\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, col_mean - z_cutoff, col_mean + z_cutoff)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    t0 = time.perf_counter()\n",
    "    df[\"year\"] = df['local_datetime'].dt.year\n",
    "    df[\"month\"] = df['local_datetime'].dt.month\n",
    "    df = select_best_features(df)  # remove columns based on previous results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features\n",
    "    df = interpolate_and_clip_outliers(df, cutoff=2)  # deal outliers using z statistics\n",
    "    df = compute_ewm_features(df)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    df = df.dropna().iloc[-(8+48):]   # Shifted 48\n",
    "    df_numy= df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1).to_numpy()\n",
    "    \n",
    "    return df_numy\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['SettlementTime', 'SP' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'ImbalanceQuantity(MAW)(B1780)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    print(prefix_path)\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file here\")\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SP'] = range(0, len(df))\n",
    "        df['SP'] = df['SP'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'intraday_pred',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "def override_result(df: pd.DataFrame, response_arr):\n",
    "    time_now = datetime.now()\n",
    "    time_down = time_now - timedelta(minutes=30)\n",
    "    df = df[list_cols]\n",
    "    index = df.index[(df['SettlementTime'] > time_down) & (df['SettlementTime'] < time_now)].to_list()[0]\n",
    "    print ('index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(response_arr)):\n",
    "        df.at[index, 'niv_predicted_{}sp'.format(i + 1)] = response_arr[i]\n",
    "        print(index, response_arr[i], '\\n')\n",
    "        index += 1\n",
    "    return df\n",
    "\n",
    "def get_scaler(resource, bucket, file_key=\"\", model_type=\"intraday\"):\n",
    "    \"\"\"Gets the scaler used during training from S3\"\"\"\n",
    "    prefix = \"{}/{}/lgbm-{}\".format(\n",
    "        model_type.split(\"-\")[0], \n",
    "        strftime('%Y/%m', gmtime()), model_type\n",
    "    )\n",
    "    if not file_key:\n",
    "        # get the most recent trained model outputs\n",
    "        list_files = []\n",
    "        for object_summary in bucket.objects.filter(Prefix=prefix):\n",
    "            list_files.append(object_summary.key)\n",
    "        list_files = [x for x in list_files if \"output/output\" in x]\n",
    "        sorted_files = sorted([x.split(prefix)[1] for x in list_files])\n",
    "        file_name = prefix + sorted_files[-1]\n",
    "        print(\"Output files available:\", sorted_files)\n",
    "        print(\"File chosen:\", file_name)\n",
    "        # read tarfile from S3 location defined before\n",
    "        fileobj = BytesIO(resource.Object(bucket.name, file_name).get()[\"Body\"].read())\n",
    "        with tarfile.open(fileobj=fileobj) as tarf:\n",
    "            # only 1 scaler is outputted per training session\n",
    "            scaler_file_name = [f.name for f in tarf if \"scaler\" in f.name.lower()][0]\n",
    "            scaler = joblib.load(tarf.extractfile(scaler_file_name))\n",
    "    else:\n",
    "        # read tarfile from S3 location defined in arguments passed during call\n",
    "        fileobj = BytesIO(resource.Object(bucket.name, file_key).get()[\"Body\"].read())\n",
    "        with tarfile.open(fileobj=fileobj) as tarf:\n",
    "            # only 1 scaler is outputted per training session\n",
    "            scaler_file_name = [f.name for f in tarf if \"scaler\" in f.name.lower()][0]\n",
    "            scaler = joblib.load(tarf.extractfile(scaler_file_name))\n",
    "    return scaler\n",
    "        \n",
    "def lambda_handler(event, context): #event, context\n",
    "#     try:\n",
    "    \n",
    "    ### Initialize\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = 'fpt-results-storage'\n",
    "    pred_folder = 'harry'\n",
    "    bucket = s3.Bucket(my_bucket)\n",
    "    bucket_model = s3.Bucket('lgbm-model-storage')\n",
    "    resource = boto3.resource('s3')\n",
    "    read_path = \"s3://scgc/data/merged\"\n",
    "    write_path = \"s3://\" + my_bucket\n",
    "    today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    next_day = (dt.datetime.now() + dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    prev_day = (dt.datetime.now() - dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    ### Get predict Data\n",
    "    merged_df = pd.DataFrame()\n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = next_day,\n",
    "        path = read_path,\n",
    "    )\n",
    "\n",
    "    ### Pre-process Data\n",
    "\n",
    "    print(\"Pre-Processing...............\")\n",
    "    payload = preprocess_dataframe(merged_df)\n",
    "    print(\"Ending reading Payload!\")\n",
    "\n",
    "    scaler = get_scaler(resource, bucket_model, model_type=\"intraday\")\n",
    "    payload = scaler.transform(payload)\n",
    "    ### Invoke Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with payload data\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-inference\",\n",
    "        ContentType=\"application/JSON\",\n",
    "        TargetModel=\"lgbm-intraday.tar.gz\",\n",
    "        Body=json.dumps(payload.tolist()), \n",
    "    )\n",
    "    arr = json.loads(response[\"Body\"].read())\n",
    "    arr = arr[-8:]\n",
    "\n",
    "    ### Get old result Data, check exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, next_day, cols=list_cols)\n",
    "\n",
    "    ### Read result file\n",
    "    result_combine = read_parquet_tables(\n",
    "        file_name= \"intraday_pred\",\n",
    "        start_date = prev_day,\n",
    "        end_date = next_day,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "\n",
    "    ### Override the result & NIV insert\n",
    "    result_file = override_result(result_combine, arr)\n",
    "    niv_value, niv_time = get_niv(merged_df)\n",
    "    print (niv_value, niv_time, ' \\n')\n",
    "    niv_inx = result_file.index[(result_file['SettlementTime'] > niv_time - timedelta(minutes=10))\n",
    "                                & (result_file['SettlementTime'] < niv_time + timedelta(minutes=10))].to_list()[0]  # index.name, index.values\n",
    "    result_file.at[niv_inx, 'ImbalanceQuantity(MAW)(B1780)'] = niv_value\n",
    "    print (\"Lasted NIV Index in Result file : \", niv_inx, ', at ', niv_time)\n",
    "\n",
    "    #### Divide and deliver to S3 Result\n",
    "    result_file['SettlementTime'] = pd.to_datetime(result_file['SettlementTime'], format='%Y-%m-%d')\n",
    "\n",
    "    prev_data = result_file[result_file['SettlementTime'] < dt.datetime.strptime(today, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = prev_day\n",
    "    )\n",
    "\n",
    "    now_data = result_file[(result_file['SettlementTime'] >= dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "        & (result_file['SettlementTime'] < dt.datetime.strptime(next_day, \"%Y-%m-%d\"))]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result_file[result_file['SettlementTime'] >= dt.datetime.strptime(next_day, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = next_day\n",
    "    )\n",
    "    print(\"Successfull\")\n",
    "\n",
    "lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e6e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e53a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16663a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Version 1.2 26/7 ########################################################\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "import joblib\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "#         basename_template = f\"{file_name}.parquet\"\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + dt.timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # parse weekday information\n",
    "    dataframe[\"weekday\"] = dataframe[\"SettlementDate\"].apply(lambda x: x.isoweekday())\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"month\"] * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "    dataframe.drop([\"local_datetime\",\n",
    "        \"month\",  # information is already encoded\n",
    "        \"SettlementPeriod\",  # information is present in hour already, so remove\n",
    "        \"year\",\n",
    "        \"weekday\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_and_clip_outliers(dataframe, cutoff=3):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) / col_std > cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                z_cutoff = cutoff * col_std\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, col_mean - z_cutoff, col_mean + z_cutoff)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    t0 = time.perf_counter()\n",
    "    df[\"year\"] = df['local_datetime'].dt.year\n",
    "    df[\"month\"] = df['local_datetime'].dt.month\n",
    "    df = select_best_features(df)  # remove columns based on previous results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features\n",
    "    df = interpolate_and_clip_outliers(df, cutoff=2)  # deal outliers using z statistics\n",
    "    df = compute_ewm_features(df)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    df = df.dropna().iloc[-(8+48):]   # Shifted 48\n",
    "    df_numy= df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1).to_numpy()\n",
    "    \n",
    "    return df_numy\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['SettlementTime', 'SP' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    print(prefix_path)\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file here\")\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SP'] = range(0, len(df))\n",
    "        df['SP'] = df['SP'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'intraday_pred',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override_result(df: pd.DataFrame, niv_time, response_arr):\n",
    "    df = df[list_cols]\n",
    "    index = df.index[(df['SettlementTime'] > niv_time + timedelta(minutes=20)) & (df['SettlementTime'] < niv_time + timedelta(minutes=40))].to_list()[0]\n",
    "    print ('index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(response_arr)):\n",
    "        df.at[index, 'niv_predicted_{}sp'.format(i + 1)] = response_arr[i]\n",
    "        print(index, response_arr[i], '\\n')\n",
    "        index += 1\n",
    "    return df\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['SettlementTime'] > stime - timedelta(minutes=10))\n",
    "                                & (df['SettlementTime'] < stime + timedelta(minutes=10))].values[0]  # index.name, index.values\n",
    "    df.at[inx, column_name] = value\n",
    "    print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "def get_scaler(resource, bucket, file_key=\"\", model_type=\"intraday\"):\n",
    "    \"\"\"Gets the scaler used during training from S3\"\"\"\n",
    "    prefix = \"{}/{}/lgbm-{}\".format(\n",
    "        model_type.split(\"-\")[0], \n",
    "        strftime('%Y/%m', gmtime()), model_type\n",
    "    )\n",
    "    if not file_key:\n",
    "        # get the most recent trained model outputs\n",
    "        list_files = []\n",
    "        for object_summary in bucket.objects.filter(Prefix=prefix):\n",
    "            list_files.append(object_summary.key)\n",
    "        list_files = [x for x in list_files if \"output/output\" in x]\n",
    "        sorted_files = sorted([x.split(prefix)[1] for x in list_files])\n",
    "        file_name = prefix + sorted_files[-1]\n",
    "        print(\"Output files available:\", sorted_files)\n",
    "        print(\"File chosen:\", file_name)\n",
    "        # read tarfile from S3 location defined before\n",
    "        fileobj = BytesIO(resource.Object(bucket.name, file_name).get()[\"Body\"].read())\n",
    "        with tarfile.open(fileobj=fileobj) as tarf:\n",
    "            # only 1 scaler is outputted per training session\n",
    "            scaler_file_name = [f.name for f in tarf if \"scaler\" in f.name.lower()][0]\n",
    "            scaler = joblib.load(tarf.extractfile(scaler_file_name))\n",
    "    else:\n",
    "        # read tarfile from S3 location defined in arguments passed during call\n",
    "        fileobj = BytesIO(resource.Object(bucket.name, file_key).get()[\"Body\"].read())\n",
    "        with tarfile.open(fileobj=fileobj) as tarf:\n",
    "            # only 1 scaler is outputted per training session\n",
    "            scaler_file_name = [f.name for f in tarf if \"scaler\" in f.name.lower()][0]\n",
    "            scaler = joblib.load(tarf.extractfile(scaler_file_name))\n",
    "    return scaler\n",
    "        \n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    ### Initialize\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = 'fpt-results-storage'\n",
    "    pred_folder = 'harry'\n",
    "    bucket = s3.Bucket(my_bucket)\n",
    "    bucket_model = s3.Bucket('lgbm-model-storage')\n",
    "    resource = boto3.resource('s3')\n",
    "    read_path = \"s3://scgc/data/merged\"\n",
    "    write_path = \"s3://\" + my_bucket\n",
    "    today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    next_day = (dt.datetime.now() + dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    prev_day = (dt.datetime.now() - dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    ### Get predict Data\n",
    "    merged_df = pd.DataFrame()\n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = next_day,\n",
    "        path = read_path,\n",
    "    )\n",
    "\n",
    "    ### Pre-process Data\n",
    "\n",
    "    print(\"Pre-Processing...............\")\n",
    "    payload = preprocess_dataframe(merged_df)\n",
    "    print(\"Ending reading Payload!\")\n",
    "\n",
    "    scaler = get_scaler(resource, bucket_model, model_type=\"intraday\")\n",
    "    payload = scaler.transform(payload)\n",
    "    ### Invoke Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with payload data\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-inference\",\n",
    "        ContentType=\"application/JSON\",\n",
    "        TargetModel=\"lgbm-intraday.tar.gz\",\n",
    "        Body=json.dumps(payload.tolist()), \n",
    "    )\n",
    "    arr = json.loads(response[\"Body\"].read())\n",
    "    arr = arr[-8:]\n",
    "\n",
    "    ### Get old\n",
    "    result Data, check exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, prev_day, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, next_day, cols=list_cols)\n",
    "\n",
    "    ### Read result file\n",
    "    result_file = read_parquet_tables(\n",
    "        file_name= \"intraday_pred\",\n",
    "        start_date = prev_day,\n",
    "        end_date = next_day,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    \n",
    "    ### Override the result & NIV insert & MID insert\n",
    "    niv_value, niv_time = get_lastest_value(merged_df, 'ImbalanceQuantity(MAW)(B1780)')\n",
    "    result_file = override(result_file, 'ImbalanceQuantity(MAW)(B1780)', niv_value, niv_time)\n",
    "    \n",
    "    mid_value, mid_time = get_lastest_value(merged_df, 'marketIndexPrice(MID)')\n",
    "    result_file = override(result_file, 'marketIndexPrice(MID)', mid_value, mid_time)\n",
    "    \n",
    "    ipa_value, ipa_time = get_lastest_value(merged_df, 'ImbalancePriceAmount(B1770)')\n",
    "    result_file = override(result_file, 'ImbalancePriceAmount(B1770)', ipa_value, ipa_time)\n",
    "    \n",
    "    result_file = override_result(result_file, niv_time, arr)\n",
    "    \n",
    "    #### Divide and deliver to S3 Result\n",
    "    result_file['SettlementTime'] = pd.to_datetime(result_file['SettlementTime'], format='%Y-%m-%d')\n",
    "    \n",
    "    if '__index_level_0__' in result_file.columns:\n",
    "        result_file.drop('__index_level_0__', axis = 1)\n",
    "    \n",
    "    prev_data = result_file[result_file['SettlementTime'] < dt.datetime.strptime(today, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = prev_day\n",
    "    )\n",
    "\n",
    "    now_data = result_file[(result_file['SettlementTime'] >= dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "        & (result_file['SettlementTime'] < dt.datetime.strptime(next_day, \"%Y-%m-%d\"))]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result_file[result_file['SettlementTime'] >= dt.datetime.strptime(next_day, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = next_day\n",
    "    )\n",
    "    print(\"Successfull\")\n",
    "# lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Version 2.0 29/7 ########################################################\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "#         basename_template = f\"{file_name}.parquet\"\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + dt.timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"local_datetime\"].dt.month * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "    dataframe.drop([\"local_datetime\",  # information is already encoded\n",
    "        \"SettlementPeriod\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_outliers(dataframe, cutoff=2.5):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        for idx in range(len(col)):\n",
    "            if np.abs(col[idx] - col_mean) > z_cutoff:\n",
    "                try:\n",
    "                    dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                except IndexError:\n",
    "                    # this only happens at either end of the input data, so we leave the value as is\n",
    "                    # it will be processed in the clip_outliers function if the cutoff there is less than or\n",
    "                    # equal to the cutoff here\n",
    "                    pass\n",
    "    return dataframe\n",
    "\n",
    "def clip_outliers(dataframe, cutoff=2):\n",
    "    \"\"\"Clip outlier using z-statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        lower_bound = col_mean - z_cutoff\n",
    "        upper_bound = col_mean + z_cutoff\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) > z_cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    t0 = time.perf_counter()\n",
    "    df = select_best_features(df)  # remove columns based on previous results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features \n",
    "    df = interpolate_outliers(df, cutoff=2.5)  # interpolate using z statistics (avg next/before obs)\n",
    "    df = clip_outliers(df, cutoff=2)  # clip outliers using z statistics\n",
    "    df = compute_ewm_features(df)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    # print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    df = df.dropna()\n",
    "    df = df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['SettlementTime', 'SP' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + dt.timedelta(days=1)\n",
    "    \n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file in \", prefix_path)\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SP'] = range(0, len(df))\n",
    "        df['SP'] = df['SP'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'intraday_pred',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override_intraday_result(df: pd.DataFrame, niv_time, response_arr):\n",
    "    \"\"\"\n",
    "    df: concatenated result files from yesterday, today & tomorrow\n",
    "    niv_time: lastest NIV updates\n",
    "    response_arr: 8 prediction values\n",
    "    \"\"\"\n",
    "    df = df[list_cols]\n",
    "    index = df.index[(df['SettlementTime'] > niv_time + timedelta(minutes=20)) \n",
    "                     &(df['SettlementTime'] < niv_time + timedelta(minutes=40))].values[0]\n",
    "    print ('Intraday index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(response_arr)):\n",
    "        index += 1\n",
    "        df.at[index, 'niv_predicted_{}sp'.format(i + 1)] = response_arr[i]\n",
    "        print(index, response_arr[i], '\\n')\n",
    "    return df\n",
    "\n",
    "def override_dayahead_result(df: pd.DataFrame, \n",
    "                             dayahead_payload: np.array,\n",
    "                             niv_time: dt.datetime,\n",
    "                            ):\n",
    "    \"\"\"\n",
    "    df: concatenated result files from yesterday, today & tomorrow\n",
    "    dayahead_payload: All Payload from yesterday, today & tomorrow\n",
    "    \"\"\"\n",
    "    if (datetime.now().hour == 8 or datetime.now().hour == 14) & (datetime.now().minute > 30):\n",
    "        df = df[list_cols]\n",
    "        print (\"Length of dayahead_payload: \", len(dayahead_payload))\n",
    "        ### Invoke DAYAHEAD Endpoint & Get Result\n",
    "        half_day = 'morning' if datetime.now().hour < 10 else 'afternoon'\n",
    "        sp_offset = (niv_time.hour * 2) + (niv_time.minute and 1) # Get number \n",
    "        print (\"Number of SP offset : \", sp_offset)\n",
    "        dayahead_payload[-(48 + 96 + sp_offset):]\n",
    "        \n",
    "        \n",
    "        ### Get the newest model file aka \"TargetModel\"\n",
    "        list_files = []\n",
    "        for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "            if f\"lgbm-regressor-dayahead-{half_day}\" in object_summary.key:\n",
    "                list_files.append(object_summary.key)\n",
    "        assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "        target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "        print(\"Lastes Day-Ahead target Model :\", target_model)\n",
    "        \n",
    "        runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "        print (\"Invoking endpoint with Day-Ahead data in the\", half_day)\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=\"lgbm-regressor-endpoint\",\n",
    "            ContentType =\"application/JSON\",\n",
    "            TargetModel =target_model,\n",
    "            Body=json.dumps(dayahead_payload.tolist()), \n",
    "        )\n",
    "        dayhead_arr = json.loads(response[\"Body\"].read())\n",
    "        dayhead_arr = dayhead_arr[-(48 + sp_offset + 2):]\n",
    "        dayhead_arr = dayhead_arr[:48]\n",
    "    \n",
    "        # Insert Value\n",
    "        index = 48 + 46 ### SP = 47 so index = 46 & add 48 from whole yesterday\n",
    "        print('Length of dayahead array: ', len(dayhead_arr))\n",
    "        for i in range(len(dayhead_arr)):\n",
    "            df.at[index, 'dayahead_{}'.format(half_day)] = dayhead_arr[i]\n",
    "            index += 1\n",
    "        # df.loc[index:, 'dayahead_{}'.format(half_day)] = dayhead_arr\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['SettlementTime'] > stime - timedelta(minutes=10))\n",
    "                   &(df['SettlementTime'] < stime + timedelta(minutes=10))].values[0] \n",
    "    df.at[inx, column_name] = value\n",
    "    # print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "\n",
    "################################# Initialize ########################################################\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = 'niv-predictions'\n",
    "pred_folder = 'lgbm-prediction'\n",
    "bucket = s3.Bucket(my_bucket)\n",
    "model_bucket = s3.Bucket('lgbm-model-storage')\n",
    "resource = boto3.resource('s3')\n",
    "read_path = \"s3://scgc/data/merged\"\n",
    "write_path = \"s3://\" + my_bucket\n",
    "prev_day = (dt.datetime.now() - dt.timedelta(days=4)).strftime(\"%Y-%m-%d\")\n",
    "yesterday= (dt.datetime.now() - dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "tomorrow = (dt.datetime.now() + dt.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    ################################# Get & Pre-process Data #############################################\n",
    "    ### Get Data \n",
    "    merged_df = pd.DataFrame()\n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = tomorrow,\n",
    "        path = read_path,\n",
    "    )\n",
    "    print (\"Shape of merged_df: \", merged_df.shape)\n",
    "    ### Pre-Process\n",
    "    print(\"Pre-Processing...............\")\n",
    "    preprocessed_df  = preprocess_dataframe(merged_df)\n",
    "    print (\"Shape of preprocess_dataframe: \", preprocessed_df.shape)\n",
    "    intraday_payload = preprocessed_df.iloc[-(8+48):].to_numpy()   # Shifted 48\n",
    "    dayahead_payload = preprocessed_df.to_numpy()\n",
    "    print(\"Ended Pre-Process & got Payload\")\n",
    "    \n",
    "    ################################# Intraday #######################################################\n",
    "    ### Get the newest model file aka \"TargetModel\"\n",
    "    list_files = []\n",
    "    for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "        if \"lgbm-regressor-intraday\" in object_summary.key:\n",
    "            list_files.append(object_summary.key)\n",
    "    assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "    target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "    print(\"Lastest Intraday target Model :\", target_model)\n",
    "    \n",
    "    ### Invoke INTRADAY Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with Payload data\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-endpoint\",\n",
    "        ContentType =\"application/JSON\",\n",
    "        TargetModel = target_model,\n",
    "        Body=json.dumps(intraday_payload.tolist()),\n",
    "    )\n",
    "    intraday_arr = json.loads(response[\"Body\"].read())\n",
    "    intraday_arr = intraday_arr[-8:]\n",
    "\n",
    "    ### Get old result Data, check exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, tomorrow, cols=list_cols)\n",
    "\n",
    "    ### Read result file\n",
    "    result_file = read_parquet_tables(\n",
    "        file_name= \"intraday_pred\",\n",
    "        start_date = yesterday,\n",
    "        end_date = tomorrow,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    print(\"Shape of Result_file \", result_file.shape)\n",
    "    \n",
    "    ### Override the result & insert lastest NIV, MID, price_act(DAA), ImbalancePriceAmount(B1770), \n",
    "    niv_value, niv_time = get_lastest_value(merged_df, 'ImbalanceQuantity(MAW)(B1780)')\n",
    "    result_file = override(result_file, 'ImbalanceQuantity(MAW)(B1780)', niv_value, niv_time)\n",
    "    \n",
    "    mid_value, mid_time = get_lastest_value(merged_df, 'marketIndexPrice(MID)')\n",
    "    result_file = override(result_file, 'marketIndexPrice(MID)', mid_value, mid_time)\n",
    "    \n",
    "    ipa_value, ipa_time = get_lastest_value(merged_df, 'ImbalancePriceAmount(B1770)')\n",
    "    result_file = override(result_file, 'ImbalancePriceAmount(B1770)', ipa_value, ipa_time)\n",
    "    \n",
    "    price_value, price_time = get_lastest_value(merged_df, 'price_act(DAA)')\n",
    "    result_file = override(result_file, 'price_act(DAA)', price_value, price_time)\n",
    "    \n",
    "    \n",
    "    result_file = override_intraday_result(result_file, niv_time, intraday_arr)\n",
    "    \n",
    "    if '__index_level_0__' in result_file.columns:\n",
    "        result_file.drop('__index_level_0__', axis = 1)\n",
    "    \n",
    "    ################################# Day-Ahead #######################################################\n",
    "    result_file = override_dayahead_result(result_file, dayahead_payload, niv_time)\n",
    "    \n",
    "    #### Divide and deliver to S3 Result\n",
    "    result_file['SettlementTime'] = pd.to_datetime(result_file['SettlementTime'], format='%Y-%m-%d')\n",
    "    \n",
    "    if '__index_level_0__' in result_file.columns:\n",
    "        result_file.drop('__index_level_0__', axis = 1)\n",
    "        \n",
    "    ################################# Output to file #######################################################\n",
    "    \n",
    "    prev_data = result_file[result_file['SettlementTime'] < dt.datetime.strptime(today, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = yesterday\n",
    "    )\n",
    "\n",
    "    now_data = result_file[(result_file['SettlementTime'] >= dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "        & (result_file['SettlementTime'] < dt.datetime.strptime(tomorrow, \"%Y-%m-%d\"))]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result_file[result_file['SettlementTime'] >= dt.datetime.strptime(tomorrow, \"%Y-%m-%d\")]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'intraday_pred',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = tomorrow\n",
    "    )\n",
    "    print(\"Successfull\")\n",
    "# lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Version 3.0 03/8 ########################################################\n",
    "\n",
    "import datetime \n",
    "from datetime import (datetime,\n",
    "                      timedelta)\n",
    "import time\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import gc\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import traceback\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def truncate_to_dayahead_format(dataframe, date_time):\n",
    "    \"\"\"\n",
    "    Function for post-processing prediction returned. Returns a dataframe of\n",
    "    prediction that contains the predictions for end of today up to tomorrow:\n",
    "        ie. from SP_47 TODAY -> SP_46 of TOMORROW inclusive\n",
    "    Args:\n",
    "        dataframe: pd.DataFrame containing local_datetime and NIVPredictions\n",
    "        date_time: datetime chosen\n",
    "\n",
    "    Returns:\n",
    "        truncated pd.DataFrame\n",
    "    \"\"\"\n",
    "    now = date_time\n",
    "    tmr = date_time + timedelta(days=1)\n",
    "    start_pred = dataframe[dataframe[\"local_datetime\"].dt.day == now.day].iloc[-2].name\n",
    "    end_pred = dataframe[dataframe[\"local_datetime\"].dt.day == tmr.day].iloc[-3].name\n",
    "    dataframe_ = dataframe.loc[start_pred:end_pred]  # use loc because we're using index name\n",
    "    if dataframe_[\"NIVPredictions\"].isna().sum():\n",
    "        raise Exception(\"NIVPredictions has NaN after post-processing. \"\n",
    "                        \"Consider raising the backshift_niv_by hyperparameter\"\n",
    "                        \"during training\")\n",
    "\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def split_file_ands_save_to_s3(result: pd.DataFrame, datetime_val: dt.datetime):\n",
    "    \"\"\"\n",
    "    Split dt to 3 file base on date then save them to s3 \n",
    "    \n",
    "    \"\"\"\n",
    "    t_today     = datetime_val\n",
    "    t_yesterday = t_today - timedelta(days=1)\n",
    "    t_tomorrow  = t_today + timedelta(days=1)\n",
    "    \n",
    "    prev_data = result[result['local_datetime'].dt.day == t_yesterday.day]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = yesterday\n",
    "    )\n",
    "\n",
    "    now_data = result[result['local_datetime'].dt.day == t_today.day]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result[result['local_datetime'].dt.day == t_tomorrow.day]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = tomorrow\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"local_datetime\"].dt.month * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "#     dataframe.drop([\"local_datetime\",  # information is already encoded\n",
    "#         \"SettlementPeriod\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_outliers(dataframe, cutoff=2.5):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        for idx in range(len(col)):\n",
    "            if np.abs(col[idx] - col_mean) > z_cutoff:\n",
    "                try:\n",
    "                    dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                except IndexError:\n",
    "                    # this only happens at either end of the input data, so we leave the value as is\n",
    "                    # it will be processed in the clip_outliers function if the cutoff there is less than or\n",
    "                    # equal to the cutoff here\n",
    "                    pass\n",
    "    return dataframe\n",
    "\n",
    "def clip_outliers(dataframe, cutoff=2):\n",
    "    \"\"\"Clip outlier using z-statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        lower_bound = col_mean - z_cutoff\n",
    "        upper_bound = col_mean + z_cutoff\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) > z_cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    t0 = time.perf_counter()\n",
    "    df = select_best_features(df)  # remove columns based on previous results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features \n",
    "    df = interpolate_outliers(df, cutoff=2.5)  # interpolate using z statistics (avg next/before obs)\n",
    "    df = clip_outliers(df, cutoff=2)  # clip outliers using z statistics\n",
    "    df = compute_ewm_features(df)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    # print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    df = df.dropna()\n",
    "    # df = df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['local_datetime', 'SettlementPeriod' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    \n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file in \", prefix_path)\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SettlementPeriod'] = range(0, len(df))\n",
    "        df['SettlementPeriod'] = df['SettlementPeriod'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'prediction',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['local_datetime'] > stime - timedelta(minutes=10))\n",
    "                   &(df['local_datetime'] < stime + timedelta(minutes=10))].values[0] \n",
    "    df.at[inx, column_name] = value\n",
    "    # print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "\n",
    "def day_ahead(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    if (datetime.now().hour == 8 or datetime.now().hour == 14) & (datetime.now().minute > 30):\n",
    "        # split target out of payload input\n",
    "        dayahead_payload = processed_df.drop([\n",
    "            \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "            \"local_datetime\",\n",
    "            \"SettlementPeriod\",\n",
    "        ], axis=1).to_numpy()\n",
    "\n",
    "        half_day = 'morning' if datetime.now().hour < 12 else 'afternoon'\n",
    "        print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "\n",
    "        ##### invoke model endpoint for getting predictions #####\n",
    "        ### Get the newest model file aka \"TargetModel\"\n",
    "        list_files = []\n",
    "        for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "            if f\"lgbm-regressor-dayahead-{half_day}\" in object_summary.key:\n",
    "                list_files.append(object_summary.key)\n",
    "        assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "        target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "        print(\"Lastest Day-Ahead target Model :\", target_model)\n",
    "\n",
    "        ### Invoke DAY-AHEAD Endpoint & Get Result\n",
    "        runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "        print (\"Invoking day-ahead endpoint with Payload data\")\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=\"lgbm-regressor-endpoint\",\n",
    "            ContentType =\"application/JSON\",\n",
    "            TargetModel = target_model,\n",
    "            Body=json.dumps(dayahead_payload.tolist()),\n",
    "        )\n",
    "        dayahead_arr = json.loads(response[\"Body\"].read())\n",
    "        print(\"Success Invoking Endpoint!\")\n",
    "\n",
    "\n",
    "        # store the prediction in a new df so that we can do some post processing\n",
    "        # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "        tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "        tmp_result_df[\"NIVPredictions\"] = dayahead_arr\n",
    "        tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "        # make a dummy df so that we can shift the predictions\n",
    "        dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "        dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "            freq=\"30T\"\n",
    "        )\n",
    "        dummy_df[\"NIVPredictions\"] = np.nan\n",
    "        dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                             processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "        dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "        # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "        tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "        tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(96)\n",
    "        tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "        print(tmp_result_df.shape)\n",
    "        # truncate the predictions to the datetime range that we want, which is\n",
    "\n",
    "        tmp_result_df = truncate_to_dayahead_format(tmp_result_df, datetime.now())\n",
    "        dayahead_arr = tmp_result_df[\"NIVPredictions\"].tolist()                             \n",
    "\n",
    "        index = 48 + 46 ### SP = 47 so index = 46 & add 48 from whole yesterday\n",
    "        print('Length of dayahead array: ', len(dayahead_arr))\n",
    "        for i in range(len(dayahead_arr)):\n",
    "            result_files.at[index, 'dayahead_{}'.format(half_day)] = dayahead_arr[i]\n",
    "            index += 1\n",
    "\n",
    "        return result_files\n",
    "    else: \n",
    "        return result_files\n",
    "\n",
    "\n",
    "def intraday(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    # split target out of payload input\n",
    "    intraday_payload = processed_df.drop([\n",
    "        \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "        \"local_datetime\",\n",
    "        \"SettlementPeriod\",\n",
    "    ], axis=1).to_numpy()\n",
    "    \n",
    "    print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "    \n",
    "    ##### Invoke model endpoint for getting predictions #####\n",
    "    ### Get the newest model file aka \"TargetModel\"\n",
    "    list_files = []\n",
    "    for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "        if f\"lgbm-regressor-intraday\" in object_summary.key:\n",
    "            list_files.append(object_summary.key)\n",
    "    assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "    target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "    print(\"Lastest Intraday target Model :\", target_model)\n",
    "    \n",
    "    ### Invoke INTRADAY Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with Intraday Payload\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-endpoint\",\n",
    "        ContentType =\"application/JSON\",\n",
    "        TargetModel = target_model,\n",
    "        Body=json.dumps(intraday_payload.tolist()),\n",
    "    )\n",
    "    intraday_arr = json.loads(response[\"Body\"].read())\n",
    "    print(\"Success Invoking Intraday Endpoint!\")\n",
    "    \n",
    "    \n",
    "    # store the prediction in a new df so that we can do some post processing\n",
    "    # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "    tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "    tmp_result_df[\"NIVPredictions\"] = intraday_arr\n",
    "    tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "    # make a dummy df so that we can shift the predictions\n",
    "    dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "    dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "        freq=\"30T\"\n",
    "    )\n",
    "    dummy_df[\"NIVPredictions\"] = np.nan\n",
    "    dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                         processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "    dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "    # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "    tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "    tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(48)\n",
    "    tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "    print(\"Shape of Result: \", tmp_result_df.shape)\n",
    "    \n",
    "    #### Get Lastest NIV Time to identify where to fill the result\n",
    "    niv_val, niv_time = get_lastest_value(processed_df, \"ImbalanceQuantity(MAW)(B1780)\")\n",
    "    print(\"Lastest NIV : \", niv_val, niv_time)\n",
    "    \n",
    "    tmp_result_df = tmp_result_df[(tmp_result_df['local_datetime'] > niv_time) & \n",
    "                                  (tmp_result_df['local_datetime'] < niv_time + timedelta(hours = 4.5))]\n",
    "    print(tmp_result_df)\n",
    "    intraday_arr = tmp_result_df[\"NIVPredictions\"].tolist()   \n",
    "    print('Length of intraday array: ', len(intraday_arr))\n",
    "    \n",
    "    # index = result_files.index[result_files['SettlementTime'].dt.datetime == niv_time].values[0]\n",
    "    index = result_files.index[(result_files['local_datetime'] > niv_time + timedelta(minutes=20))\n",
    "                               &(result_files['local_datetime'] < niv_time + timedelta(minutes=40))].values[0]\n",
    "    \n",
    "    print ('Intraday index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(intraday_arr)):\n",
    "        index += 1\n",
    "        result_files.at[index, 'niv_predicted_{}sp'.format(i + 1)] = intraday_arr[i]\n",
    "        print(index, result_files.at[index, 'local_datetime'], intraday_arr[i], '\\n')\n",
    "\n",
    "    return result_files\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = tomorrow,\n",
    "        path = read_path,\n",
    "    )\n",
    "    print (\"length of Merged dataframe: \", merged_df.shape)\n",
    "    merged_df[\"local_datetime\"] = pd.to_datetime(merged_df[\"local_datetime\"])\n",
    "    df = preprocess_dataframe(merged_df)\n",
    "    \n",
    "    ### Check result exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, tomorrow, cols=list_cols)\n",
    "    \n",
    "    result_files = read_parquet_tables(\n",
    "        file_name= \"prediction\", \n",
    "        start_date = yesterday,\n",
    "        end_date = tomorrow,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    print(\"Shape of Result_file \", result_files.shape)\n",
    "    if 'SettlementTime' in result_files.columns:\n",
    "        result_files.rename(columns = {'SettlementTime':'local_datetime', 'SP':'SettlementPeriod'}, inplace = True)\n",
    "    \n",
    "    niv_value, niv_time = get_lastest_value(merged_df, 'ImbalanceQuantity(MAW)(B1780)')\n",
    "    result_files = override(result_files, 'ImbalanceQuantity(MAW)(B1780)', niv_value, niv_time)    \n",
    "    \n",
    "    mid_value, mid_time = get_lastest_value(merged_df, 'marketIndexPrice(MID)')\n",
    "    result_files = override(result_files, 'marketIndexPrice(MID)', mid_value, mid_time)\n",
    "    \n",
    "    ipa_value, ipa_time = get_lastest_value(merged_df, 'ImbalancePriceAmount(B1770)')\n",
    "    result_files = override(result_files, 'ImbalancePriceAmount(B1770)', ipa_value, ipa_time)\n",
    "    \n",
    "    price_value, price_time = get_lastest_value(merged_df, 'price_act(DAA)')\n",
    "    result_files = override(result_files, 'price_act(DAA)', price_value, price_time)\n",
    "    \n",
    "    \n",
    "    result_files['local_datetime'] = pd.to_datetime(result_files[\"local_datetime\"])\n",
    "    \n",
    "    result_files = day_ahead(result_files, df)\n",
    "    result_files = intraday (result_files, df)\n",
    "    \n",
    "    ##### Delivery files to S3 \n",
    "    split_file_ands_save_to_s3(result_files, dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "\n",
    "    print('Testing successfully!')\n",
    "\n",
    "################################# Initialize ########################################################\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = 'niv-predictions'   \n",
    "pred_folder = 'lgbm-prediction' \n",
    "bucket = s3.Bucket(my_bucket)\n",
    "model_bucket = s3.Bucket('lgbm-model-storage')\n",
    "resource = boto3.resource('s3')\n",
    "read_path = \"s3://scgc/data/merged\"\n",
    "write_path = \"s3://\" + my_bucket\n",
    "prev_day = (dt.datetime.now() - timedelta(days=4)).strftime(\"%Y-%m-%d\")\n",
    "yesterday= (dt.datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "tomorrow = (dt.datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Version 3.1 Aug6 ########################################################\n",
    "\n",
    "import datetime \n",
    "from datetime import (datetime,\n",
    "                      timedelta)\n",
    "import time\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import gc\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import traceback\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def truncate_to_dayahead_format(dataframe, date_time):\n",
    "    \"\"\"\n",
    "    Function for post-processing prediction returned. Returns a dataframe of\n",
    "    prediction that contains the predictions for end of today up to tomorrow:\n",
    "        ie. from SP_47 TODAY -> SP_46 of TOMORROW inclusive\n",
    "    Args:\n",
    "        dataframe: pd.DataFrame containing local_datetime and NIVPredictions\n",
    "        date_time: datetime chosen\n",
    "\n",
    "    Returns:\n",
    "        truncated pd.DataFrame\n",
    "    \"\"\"\n",
    "    now = date_time\n",
    "    tmr = date_time + timedelta(days=1)\n",
    "    start_pred = dataframe[dataframe[\"local_datetime\"].dt.day == now.day].iloc[-2].name\n",
    "    end_pred = dataframe[dataframe[\"local_datetime\"].dt.day == tmr.day].iloc[-3].name\n",
    "    dataframe_ = dataframe.loc[start_pred:end_pred]  # use loc because we're using index name\n",
    "    if dataframe_[\"NIVPredictions\"].isna().sum():\n",
    "        raise Exception(\"NIVPredictions has NaN after post-processing. \"\n",
    "                        \"Consider raising the backshift_niv_by hyperparameter\"\n",
    "                        \"during training\")\n",
    "\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def split_file_ands_save_to_s3(result: pd.DataFrame, datetime_val: dt.datetime):\n",
    "    \"\"\"\n",
    "    Split dt to 3 file base on date then save them to s3 \n",
    "    \n",
    "    \"\"\"\n",
    "    t_today     = datetime_val\n",
    "    t_yesterday = t_today - timedelta(days=1)\n",
    "    t_tomorrow  = t_today + timedelta(days=1)\n",
    "    \n",
    "    prev_data = result[result['local_datetime'].dt.day == t_yesterday.day]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = yesterday\n",
    "    )\n",
    "\n",
    "    now_data = result[result['local_datetime'].dt.day == t_today.day]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result[result['local_datetime'].dt.day == t_tomorrow.day]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = tomorrow\n",
    "    )\n",
    "\n",
    "\n",
    "def update_actual_columns(\n",
    "    result_file : pd.DataFrame,\n",
    "    merged_file : pd.DataFrame\n",
    "):\n",
    "    columns_name = ['ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', \n",
    "                    'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "    # print (merged_file['marketIndexPrice(MID)'].to_list())\n",
    "    result_file[columns_name] = merged_file[columns_name]\n",
    "    return result_file\n",
    "\n",
    "    \n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"local_datetime\"].dt.month * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "#     dataframe.drop([\"local_datetime\",  # information is already encoded\n",
    "#         \"SettlementPeriod\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_outliers(dataframe, cutoff=2.5):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        for idx in range(len(col)):\n",
    "            if np.abs(col[idx] - col_mean) > z_cutoff:\n",
    "                try:\n",
    "                    dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                except IndexError:\n",
    "                    # this only happens at either end of the input data, so we leave the value as is\n",
    "                    # it will be processed in the clip_outliers function if the cutoff there is less than or\n",
    "                    # equal to the cutoff here\n",
    "                    pass\n",
    "    return dataframe\n",
    "\n",
    "def clip_outliers(dataframe, cutoff=2):\n",
    "    \"\"\"Clip outlier using z-statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        lower_bound = col_mean - z_cutoff\n",
    "        upper_bound = col_mean + z_cutoff\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) > z_cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    t0 = time.perf_counter()\n",
    "    df = select_best_features(df)  # remove columns based on previous results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features \n",
    "    df = interpolate_outliers(df, cutoff=2.5)  # interpolate using z statistics (avg next/before obs)\n",
    "    df = clip_outliers(df, cutoff=2)  # clip outliers using z statistics\n",
    "    df = compute_ewm_features(df)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    # print(f\"Shape of the dataframe: {df.shape}\")\n",
    "    df = df.dropna()\n",
    "    # df = df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['local_datetime', 'SettlementPeriod' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    \n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file in \", prefix_path)\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SettlementPeriod'] = range(0, len(df))\n",
    "        df['SettlementPeriod'] = df['SettlementPeriod'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'prediction',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['local_datetime'] > stime - timedelta(minutes=10))\n",
    "                   &(df['local_datetime'] < stime + timedelta(minutes=10))].values[0] \n",
    "    df.at[inx, column_name] = value\n",
    "    # print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "\n",
    "def day_ahead(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    \"\"\" \n",
    "    Morning prediction deadline: 8:45    you can run at 8:30 (and 7:30)\n",
    "    Afternoon prediction deadline: 14:30 you can run at 14:15 (and 13:15)\n",
    "    So I set it run at 8:10 (or 7:10), 14:10 (or 13:10) should meet the deadline\n",
    "    \"\"\"\n",
    "    if (datetime.now().hour in [7, 8, 13, 14]) & (datetime.now().minute < 15):\n",
    "        # split target out of payload input\n",
    "        dayahead_payload = processed_df.drop([\n",
    "            \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "            \"local_datetime\",\n",
    "            \"SettlementPeriod\",\n",
    "        ], axis=1).to_numpy()\n",
    "\n",
    "        half_day = 'morning' if datetime.now().hour < 12 else 'afternoon'\n",
    "        print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "\n",
    "        ##### invoke model endpoint for getting predictions #####\n",
    "        ### Get the newest model file aka \"TargetModel\"\n",
    "        list_files = []\n",
    "        for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "            if f\"lgbm-regressor-dayahead-{half_day}\" in object_summary.key:\n",
    "                list_files.append(object_summary.key)\n",
    "        assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "        target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "        print(\"Lastest Day-Ahead target Model :\", target_model)\n",
    "\n",
    "        ### Invoke DAY-AHEAD Endpoint & Get Result\n",
    "        runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "        print (\"Invoking day-ahead endpoint with Payload data\")\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=\"lgbm-regressor-endpoint\",\n",
    "            ContentType =\"application/JSON\",\n",
    "            TargetModel = target_model,\n",
    "            Body=json.dumps(dayahead_payload.tolist()),\n",
    "        )\n",
    "        dayahead_arr = json.loads(response[\"Body\"].read())\n",
    "        print(\"Success Invoking Endpoint!\")\n",
    "\n",
    "\n",
    "        # store the prediction in a new df so that we can do some post processing\n",
    "        # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "        tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "        tmp_result_df[\"NIVPredictions\"] = dayahead_arr\n",
    "        tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "        # make a dummy df so that we can shift the predictions\n",
    "        dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "        dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "            freq=\"30T\"\n",
    "        )\n",
    "        dummy_df[\"NIVPredictions\"] = np.nan\n",
    "        dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                             processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "        dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "        # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "        tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "        tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(96)\n",
    "        tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "        print(tmp_result_df.shape)\n",
    "        # truncate the predictions to the datetime range that we want, which is\n",
    "\n",
    "        tmp_result_df = truncate_to_dayahead_format(tmp_result_df, datetime.now())\n",
    "        dayahead_arr = tmp_result_df[\"NIVPredictions\"].tolist()                             \n",
    "\n",
    "        index = 48 + 46 ### SP = 47 so index = 46 & add 48 from whole yesterday\n",
    "        print('Length of dayahead array: ', len(dayahead_arr))\n",
    "        for i in range(len(dayahead_arr)):\n",
    "            result_files.at[index, 'dayahead_{}'.format(half_day)] = dayahead_arr[i]\n",
    "            index += 1\n",
    "\n",
    "        return result_files\n",
    "    else: \n",
    "        return result_files\n",
    "\n",
    "\n",
    "def intraday(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    # split target out of payload input\n",
    "    intraday_payload = processed_df.drop([\n",
    "        \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "        \"local_datetime\",\n",
    "        \"SettlementPeriod\",\n",
    "    ], axis=1).to_numpy()\n",
    "    \n",
    "    print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "    \n",
    "    ##### Invoke model endpoint for getting predictions #####\n",
    "    ### Get the newest model file aka \"TargetModel\"\n",
    "    list_files = []\n",
    "    for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "        if f\"lgbm-regressor-intraday\" in object_summary.key:\n",
    "            list_files.append(object_summary.key)\n",
    "    assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "    target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "    print(\"Lastest Intraday target Model :\", target_model)\n",
    "    \n",
    "    ### Invoke INTRADAY Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with Intraday Payload\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-endpoint\",\n",
    "        ContentType =\"application/JSON\",\n",
    "        TargetModel = target_model,\n",
    "        Body=json.dumps(intraday_payload.tolist()),\n",
    "    )\n",
    "    intraday_arr = json.loads(response[\"Body\"].read())\n",
    "    print(\"Success Invoking Intraday Endpoint!\")\n",
    "    \n",
    "    \n",
    "    # store the prediction in a new df so that we can do some post processing\n",
    "    # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "    tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "    tmp_result_df[\"NIVPredictions\"] = intraday_arr\n",
    "    tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "    # make a dummy df so that we can shift the predictions\n",
    "    dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "    dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "        freq=\"30T\"\n",
    "    )\n",
    "    dummy_df[\"NIVPredictions\"] = np.nan\n",
    "    dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                         processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "    dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "    # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "    tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "    tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(48)\n",
    "    tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "    print(\"Shape of Result: \", tmp_result_df.shape)\n",
    "    \n",
    "    #### Get Lastest NIV Time to identify where to fill the result\n",
    "    niv_val, niv_time = get_lastest_value(processed_df, \"ImbalanceQuantity(MAW)(B1780)\")\n",
    "    print(\"Lastest NIV : \", niv_val, niv_time)\n",
    "    \n",
    "    tmp_result_df = tmp_result_df[(tmp_result_df['local_datetime'] > niv_time) & \n",
    "                                  (tmp_result_df['local_datetime'] < niv_time + timedelta(hours = 4.5))]\n",
    "    print(tmp_result_df)\n",
    "    intraday_arr = tmp_result_df[\"NIVPredictions\"].tolist()   \n",
    "    print('Length of intraday array: ', len(intraday_arr))\n",
    "    \n",
    "    # index = result_files.index[result_files['SettlementTime'].dt.datetime == niv_time].values[0]\n",
    "    index = result_files.index[(result_files['local_datetime'] > niv_time + timedelta(minutes=20))\n",
    "                               &(result_files['local_datetime'] < niv_time + timedelta(minutes=40))].values[0]\n",
    "    \n",
    "    print ('Intraday index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(intraday_arr)):\n",
    "        index += 1\n",
    "        result_files.at[index, 'niv_predicted_{}sp'.format(i + 1)] = intraday_arr[i]\n",
    "        print(index, result_files.at[index, 'local_datetime'], intraday_arr[i], '\\n')\n",
    "\n",
    "    return result_files\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = tomorrow,\n",
    "        path = read_path,\n",
    "    )\n",
    "    print (\"length of Merged dataframe: \", merged_df.shape)\n",
    "    merged_df[\"local_datetime\"] = pd.to_datetime(merged_df[\"local_datetime\"])\n",
    "    df = preprocess_dataframe(merged_df)\n",
    "    \n",
    "    ### Check result exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, tomorrow, cols=list_cols)\n",
    "    \n",
    "    result_files = read_parquet_tables(\n",
    "        file_name= \"prediction\", \n",
    "        start_date = yesterday,\n",
    "        end_date = tomorrow,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    print(\"Shape of Result_file \", result_files.shape)\n",
    "    if 'SettlementTime' in result_files.columns:\n",
    "        result_files.rename(columns = {'SettlementTime':'local_datetime', 'SP':'SettlementPeriod'}, inplace = True) \n",
    "    \n",
    "    result_files['local_datetime'] = pd.to_datetime(result_files[\"local_datetime\"])\n",
    "    \n",
    "    result_files = day_ahead(result_files, df)\n",
    "    result_files = intraday (result_files, df)\n",
    "    \n",
    "    ### Update actual values in order to calculate PnL in the future\n",
    "    merged_file = merged_df[merged_df['local_datetime'] >= dt.datetime.strptime(yesterday, \"%Y-%m-%d\")]\n",
    "    print (\"# Row of actual data before joining to result files\", merged_file.shape[0])\n",
    "    result_files = update_actual_columns(result_files, merged_file.reset_index(drop = True))\n",
    "    \n",
    "    ##### Delivery files to S3 \n",
    "    split_file_ands_save_to_s3(result_files, dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "\n",
    "    print('Testing successfully!')\n",
    "\n",
    "################################# Initialize ########################################################\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = 'niv-predictions'   \n",
    "pred_folder = 'lgbm-prediction' \n",
    "bucket = s3.Bucket(my_bucket)\n",
    "model_bucket = s3.Bucket('lgbm-model-storage')\n",
    "resource = boto3.resource('s3')\n",
    "read_path = \"s3://scgc/data/merged\"\n",
    "write_path = \"s3://\" + my_bucket\n",
    "prev_day = (dt.datetime.now() - timedelta(days=4)).strftime(\"%Y-%m-%d\")\n",
    "yesterday= (dt.datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "tomorrow = (dt.datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75680fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Merged dataframe:  (288, 179)\n",
      "Time taken to preprocess features in seconds: 0.15955582999959006\n",
      "File Exits\n",
      "File Exits\n",
      "Shape of Result_file  (144, 16)\n",
      "length of preprocess_dataframe:  (203, 31)\n",
      "Lastest Day-Ahead target Model : lgbm-regressor-dayahead-morning-2022-08-04.tar.gz\n",
      "Invoking day-ahead endpoint with Payload data\n",
      "Success Invoking Endpoint!\n",
      "(203, 3)\n",
      "Length of dayahead array:  48\n",
      "length of preprocess_dataframe:  (203, 31)\n",
      "Lastest Intraday target Model : lgbm-regressor-intraday-2022-08-04.tar.gz\n",
      "Invoking endpoint with Intraday Payload\n",
      "Success Invoking Intraday Endpoint!\n",
      "Shape of Result:  (203, 3)\n",
      "Lastest NIV :  196.6122 2022-08-19 06:30:00\n",
      "         local_datetime  SettlementPeriod  NIVPredictions\n",
      "203 2022-08-19 07:00:00                15     -121.561811\n",
      "204 2022-08-19 07:30:00                16     -214.691102\n",
      "205 2022-08-19 08:00:00                17      -91.925530\n",
      "206 2022-08-19 08:30:00                18      -38.511855\n",
      "207 2022-08-19 09:00:00                19     -213.360719\n",
      "208 2022-08-19 09:30:00                20     -197.787593\n",
      "209 2022-08-19 10:00:00                21      -10.029272\n",
      "210 2022-08-19 10:30:00                22     -198.824158\n",
      "Length of intraday array:  8\n",
      "Intraday index =  62\n",
      "63 2022-08-19 07:30:00 -121.56181133225378 \n",
      "\n",
      "64 2022-08-19 08:00:00 -214.691102376546 \n",
      "\n",
      "65 2022-08-19 08:30:00 -91.92552983696551 \n",
      "\n",
      "66 2022-08-19 09:00:00 -38.511855363595934 \n",
      "\n",
      "67 2022-08-19 09:30:00 -213.36071851612496 \n",
      "\n",
      "68 2022-08-19 10:00:00 -197.78759330104543 \n",
      "\n",
      "69 2022-08-19 10:30:00 -10.029271572585628 \n",
      "\n",
      "70 2022-08-19 11:00:00 -198.82415766025105 \n",
      "\n",
      "# Row of actual data before joining to result files 144\n",
      "Testing successfully!\n"
     ]
    }
   ],
   "source": [
    "############################################### Version 3.2 Aug19 ########################################################\n",
    "### Edit window size of compute_ewm_features function\n",
    "\n",
    "import datetime \n",
    "from datetime import (datetime,\n",
    "                      timedelta)\n",
    "import time\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import gc\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import traceback\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def truncate_to_dayahead_format(dataframe, date_time):\n",
    "    \"\"\"\n",
    "    Function for post-processing prediction returned. Returns a dataframe of\n",
    "    prediction that contains the predictions for end of today up to tomorrow:\n",
    "        ie. from SP_47 TODAY -> SP_46 of TOMORROW inclusive\n",
    "    Args:\n",
    "        dataframe: pd.DataFrame containing local_datetime and NIVPredictions\n",
    "        date_time: datetime chosen\n",
    "\n",
    "    Returns:\n",
    "        truncated pd.DataFrame\n",
    "    \"\"\"\n",
    "    now = date_time\n",
    "    tmr = date_time + timedelta(days=1)\n",
    "    start_pred = dataframe[dataframe[\"local_datetime\"].dt.day == now.day].iloc[-2].name\n",
    "    end_pred = dataframe[dataframe[\"local_datetime\"].dt.day == tmr.day].iloc[-3].name\n",
    "    dataframe_ = dataframe.loc[start_pred:end_pred]  # use loc because we're using index name\n",
    "    if dataframe_[\"NIVPredictions\"].isna().sum():\n",
    "        raise Exception(\"NIVPredictions has NaN after post-processing. \"\n",
    "                        \"Consider raising the backshift_niv_by hyperparameter\"\n",
    "                        \"during training\")\n",
    "\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def split_file_ands_save_to_s3(result: pd.DataFrame, datetime_val: dt.datetime):\n",
    "    \"\"\"\n",
    "    Split dt to 3 file base on date then save them to s3 \n",
    "    \n",
    "    \"\"\"\n",
    "    t_today     = datetime_val\n",
    "    t_yesterday = t_today - timedelta(days=1)\n",
    "    t_tomorrow  = t_today + timedelta(days=1)\n",
    "    \n",
    "    prev_data = result[result['local_datetime'].dt.day == t_yesterday.day]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = yesterday\n",
    "    )\n",
    "\n",
    "    now_data = result[result['local_datetime'].dt.day == t_today.day]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result[result['local_datetime'].dt.day == t_tomorrow.day]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = tomorrow\n",
    "    )\n",
    "\n",
    "\n",
    "def update_actual_columns(\n",
    "    result_file : pd.DataFrame,\n",
    "    merged_file : pd.DataFrame\n",
    "):\n",
    "    columns_name = ['ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', \n",
    "                    'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "    # print (merged_file['marketIndexPrice(MID)'].to_list())\n",
    "    result_file[columns_name] = merged_file[columns_name]\n",
    "    return result_file\n",
    "\n",
    "    \n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"local_datetime\"].dt.month * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "#     dataframe.drop([\"local_datetime\",  # information is already encoded\n",
    "#         \"SettlementPeriod\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_outliers(dataframe, cutoff=2.5):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        for idx in range(len(col)):\n",
    "            if np.abs(col[idx] - col_mean) > z_cutoff:\n",
    "                try:\n",
    "                    dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                except IndexError:\n",
    "                    # this only happens at either end of the input data, so we leave the value as is\n",
    "                    # it will be processed in the clip_outliers function if the cutoff there is less than or\n",
    "                    # equal to the cutoff here\n",
    "                    pass\n",
    "    return dataframe\n",
    "\n",
    "def clip_outliers(dataframe, cutoff=2):\n",
    "    \"\"\"Clip outlier using z-statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        lower_bound = col_mean - z_cutoff\n",
    "        upper_bound = col_mean + z_cutoff\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) > z_cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    df = select_best_features(df)  # remove columns based on previous experiment results\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features\n",
    "    df = interpolate_outliers(df, cutoff=2.5)  # interpolate using z statistics (avg next/before obs)\n",
    "    df = clip_outliers(df, cutoff=2)  # clip outliers using z statistics\n",
    "    df = compute_ewm_features(df, window=4)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    \n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\",\n",
    "                  \"__index_level_0__\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    # df = df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "\n",
    "list_cols = ['local_datetime', 'SettlementPeriod' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    \n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file in \", prefix_path)\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SettlementPeriod'] = range(0, len(df))\n",
    "        df['SettlementPeriod'] = df['SettlementPeriod'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'prediction',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['local_datetime'] > stime - timedelta(minutes=10))\n",
    "                   &(df['local_datetime'] < stime + timedelta(minutes=10))].values[0] \n",
    "    df.at[inx, column_name] = value\n",
    "    # print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "\n",
    "def day_ahead(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    \"\"\" \n",
    "    Morning prediction deadline: 8:45    you can run at 8:30 (and 7:30)\n",
    "    Afternoon prediction deadline: 14:30 you can run at 14:15 (and 13:15)\n",
    "    So I set it run at 8:10 (or 7:10), 14:10 (or 13:10) should meet the deadline\n",
    "    \"\"\"\n",
    "    if (datetime.now().hour in [7, 8, 13, 14]) & (datetime.now().minute < 15):\n",
    "        # split target out of payload input\n",
    "        dayahead_payload = processed_df.drop([\n",
    "            \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "            \"local_datetime\",\n",
    "            \"SettlementPeriod\",\n",
    "        ], axis=1).to_numpy()\n",
    "\n",
    "        half_day = 'morning' if datetime.now().hour < 12 else 'afternoon'\n",
    "        print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "\n",
    "        ##### invoke model endpoint for getting predictions #####\n",
    "        ### Get the newest model file aka \"TargetModel\"\n",
    "        list_files = []\n",
    "        for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "            if f\"lgbm-regressor-dayahead-{half_day}\" in object_summary.key:\n",
    "                list_files.append(object_summary.key)\n",
    "        assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "        target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "        print(\"Lastest Day-Ahead target Model :\", target_model)\n",
    "\n",
    "        ### Invoke DAY-AHEAD Endpoint & Get Result\n",
    "        runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "        print (\"Invoking day-ahead endpoint with Payload data\")\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=\"lgbm-regressor-endpoint\",\n",
    "            ContentType =\"application/JSON\",\n",
    "            TargetModel = target_model,\n",
    "            Body=json.dumps(dayahead_payload.tolist()),\n",
    "        )\n",
    "        dayahead_arr = json.loads(response[\"Body\"].read())\n",
    "        print(\"Success Invoking Endpoint!\")\n",
    "\n",
    "\n",
    "        # store the prediction in a new df so that we can do some post processing\n",
    "        # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "        tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "        tmp_result_df[\"NIVPredictions\"] = dayahead_arr\n",
    "        tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "        # make a dummy df so that we can shift the predictions\n",
    "        dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "        dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "            freq=\"30T\"\n",
    "        )\n",
    "        dummy_df[\"NIVPredictions\"] = np.nan\n",
    "        dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                             processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "        dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "        # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "        tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "        tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(96)\n",
    "        tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "        print(tmp_result_df.shape)\n",
    "        # truncate the predictions to the datetime range that we want, which is\n",
    "\n",
    "        tmp_result_df = truncate_to_dayahead_format(tmp_result_df, datetime.now())\n",
    "        dayahead_arr = tmp_result_df[\"NIVPredictions\"].tolist()                             \n",
    "\n",
    "        index = 48 + 46 ### SP = 47 so index = 46 & add 48 from whole yesterday\n",
    "        print('Length of dayahead array: ', len(dayahead_arr))\n",
    "        for i in range(len(dayahead_arr)):\n",
    "            result_files.at[index, 'dayahead_{}'.format(half_day)] = dayahead_arr[i]\n",
    "            index += 1\n",
    "\n",
    "        return result_files\n",
    "    else: \n",
    "        return result_files\n",
    "\n",
    "\n",
    "def intraday(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    # split target out of payload input\n",
    "    intraday_payload = processed_df.drop([\n",
    "        \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "        \"local_datetime\",\n",
    "        \"SettlementPeriod\",\n",
    "    ], axis=1).to_numpy()\n",
    "    \n",
    "    print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "    \n",
    "    ##### Invoke model endpoint for getting predictions #####\n",
    "    ### Get the newest model file aka \"TargetModel\"\n",
    "    list_files = []\n",
    "    for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "        if f\"lgbm-regressor-intraday\" in object_summary.key:\n",
    "            list_files.append(object_summary.key)\n",
    "    assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "    target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "    print(\"Lastest Intraday target Model :\", target_model)\n",
    "    \n",
    "    ### Invoke INTRADAY Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with Intraday Payload\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-endpoint\",\n",
    "        ContentType =\"application/JSON\",\n",
    "        TargetModel = target_model,\n",
    "        Body=json.dumps(intraday_payload.tolist()),\n",
    "    )\n",
    "    intraday_arr = json.loads(response[\"Body\"].read())\n",
    "    print(\"Success Invoking Intraday Endpoint!\")\n",
    "    \n",
    "    \n",
    "    # store the prediction in a new df so that we can do some post processing\n",
    "    # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "    tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "    tmp_result_df[\"NIVPredictions\"] = intraday_arr\n",
    "    tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "    # make a dummy df so that we can shift the predictions\n",
    "    dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "    dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "        freq=\"30T\"\n",
    "    )\n",
    "    dummy_df[\"NIVPredictions\"] = np.nan\n",
    "    dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                         processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "    dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "    # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "    tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "    tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(48)\n",
    "    tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "    print(\"Shape of Result: \", tmp_result_df.shape)\n",
    "    \n",
    "    #### Get Lastest NIV Time to identify where to fill the result\n",
    "    niv_val, niv_time = get_lastest_value(processed_df, \"ImbalanceQuantity(MAW)(B1780)\")\n",
    "    print(\"Lastest NIV : \", niv_val, niv_time)\n",
    "    \n",
    "    tmp_result_df = tmp_result_df[(tmp_result_df['local_datetime'] > niv_time) & \n",
    "                                  (tmp_result_df['local_datetime'] < niv_time + timedelta(hours = 4.5))]\n",
    "    print(tmp_result_df)\n",
    "    intraday_arr = tmp_result_df[\"NIVPredictions\"].tolist()   \n",
    "    print('Length of intraday array: ', len(intraday_arr))\n",
    "    \n",
    "    # index = result_files.index[result_files['SettlementTime'].dt.datetime == niv_time].values[0]\n",
    "    index = result_files.index[(result_files['local_datetime'] > niv_time + timedelta(minutes=20))\n",
    "                               &(result_files['local_datetime'] < niv_time + timedelta(minutes=40))].values[0]\n",
    "    \n",
    "    print ('Intraday index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(intraday_arr)):\n",
    "        index += 1\n",
    "        result_files.at[index, 'niv_predicted_{}sp'.format(i + 1)] = intraday_arr[i]\n",
    "        print(index, result_files.at[index, 'local_datetime'], intraday_arr[i], '\\n')\n",
    "\n",
    "    return result_files\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = tomorrow,\n",
    "        path = read_path,\n",
    "    )\n",
    "    print (\"length of Merged dataframe: \", merged_df.shape)\n",
    "    merged_df[\"local_datetime\"] = pd.to_datetime(merged_df[\"local_datetime\"])\n",
    "    df = preprocess_dataframe(merged_df)\n",
    "    \n",
    "    ### Check result exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, tomorrow, cols=list_cols)\n",
    "    \n",
    "    result_files = read_parquet_tables(\n",
    "        file_name= \"prediction\", \n",
    "        start_date = yesterday,\n",
    "        end_date = tomorrow,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    print(\"Shape of Result_file \", result_files.shape)\n",
    "    if 'SettlementTime' in result_files.columns:\n",
    "        result_files.rename(columns = {'SettlementTime':'local_datetime', 'SP':'SettlementPeriod'}, inplace = True) \n",
    "    \n",
    "    result_files['local_datetime'] = pd.to_datetime(result_files[\"local_datetime\"])\n",
    "    \n",
    "    result_files = day_ahead(result_files, df)\n",
    "    result_files = intraday (result_files, df)\n",
    "    \n",
    "    ### Update actual values in order to calculate PnL in the future\n",
    "    merged_file = merged_df[merged_df['local_datetime'] >= dt.datetime.strptime(yesterday, \"%Y-%m-%d\")]\n",
    "    print (\"# Row of actual data before joining to result files\", merged_file.shape[0])\n",
    "    result_files = update_actual_columns(result_files, merged_file.reset_index(drop = True))\n",
    "    \n",
    "    ##### Delivery files to S3 \n",
    "    split_file_ands_save_to_s3(result_files, dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "\n",
    "    print('Testing successfully!')\n",
    "\n",
    "################################# Initialize ########################################################\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = 'niv-predictions'   \n",
    "pred_folder = 'lgbm-prediction' \n",
    "bucket = s3.Bucket(my_bucket)\n",
    "model_bucket = s3.Bucket('lgbm-model-storage')\n",
    "resource = boto3.resource('s3')\n",
    "read_path = \"s3://scgc/data/merged\"\n",
    "write_path = \"s3://\" + my_bucket\n",
    "prev_day = (dt.datetime.now() - timedelta(days=4)).strftime(\"%Y-%m-%d\")\n",
    "yesterday= (dt.datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "tomorrow = (dt.datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "lambda_handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec17a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Version 3.3 Aug 23 ########################################################\n",
    "### Edit method of data ingestion: assign DataFrame of selected features (line 359-367) in case new columns be added\n",
    "\n",
    "import datetime \n",
    "from datetime import (datetime,\n",
    "                      timedelta)\n",
    "import time\n",
    "from time import (strftime, \n",
    "                  perf_counter, \n",
    "                  gmtime)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from functools import partial\n",
    "import gc\n",
    "import os, sys\n",
    "import json\n",
    "import s3fs\n",
    "import io\n",
    "import boto3\n",
    "import tarfile \n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import traceback\n",
    "\n",
    "filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# ============================================= Helper Functions ============================================= #\n",
    "def save_data(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    path: str,\n",
    "    date_partition: str,\n",
    "    partition_cols=None, partition_filename_cb=None, filesystem=filesystem\n",
    "):\n",
    "    \"\"\"Write pandas DataFrame in parquet format to S3 bucket\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save\n",
    "        file_name (str): Table name\n",
    "        path (str): local or AWS S3 path to store the parquet files\n",
    "        partition_cols (list, optional): Columns used to partition the parquet files. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    date = dt.datetime.strptime(date_partition, \"%Y-%m-%d\")\n",
    "    folder = f'/year={date.year}/month={date.month}/day={date.day}'\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        pyarrow.Table.from_pandas(df),\n",
    "        path + folder,\n",
    "        filesystem=filesystem,\n",
    "        partition_cols=partition_cols,\n",
    "        partition_filename_cb=lambda x: f\"{file_name}.parquet\",\n",
    "    )\n",
    "\n",
    "def generate_dates(start_date: str, end_date: str):\n",
    "    \"\"\"Generates list of dates\n",
    "    Args:\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "    Returns:\n",
    "        List: List of dates\n",
    "    \"\"\"\n",
    "    sdate = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    edate = dt.datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "    return [\n",
    "        (sdate + timedelta(days=x)).strftime(\"%Y-%m-%d\")\n",
    "        for x in range((edate - sdate).days)\n",
    "    ]\n",
    "        \n",
    "def read_parquet_tables(\n",
    "    file_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read parquet file partitions\n",
    "    Args:\n",
    "        file_name (str): Table name\n",
    "        start_date (str): starting date to clean the data (%Y-%m-%d)\n",
    "        end_date (str): ending date to clean the data (%Y-%m-%d)\n",
    "        path (str): local or AWS S3 path to read the parquet files\n",
    "    Returns:\n",
    "        pd.DataFrame: Datframe from parquet file\n",
    "    \"\"\"\n",
    "    # convert date range to list of dates\n",
    "    date_list = generate_dates(start_date, end_date)\n",
    "    df = pd.DataFrame()\n",
    "    for read_date in date_list:\n",
    "        # convert date to integers for filters\n",
    "        r_year = dt.datetime.strptime(read_date, \"%Y-%m-%d\").year\n",
    "        r_month = dt.datetime.strptime(read_date, \"%Y-%m-%d\").month\n",
    "        r_day = dt.datetime.strptime(read_date, \"%Y-%m-%d\").day\n",
    "\n",
    "        try:\n",
    "            data = (\n",
    "                pq.ParquetDataset(\n",
    "                    path\n",
    "                    + f\"/year={r_year}/month={r_month}/day={r_day}/{file_name}.parquet\",\n",
    "                    filesystem=filesystem,\n",
    "                )\n",
    "                .read_pandas()\n",
    "                .to_pandas()\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def truncate_to_dayahead_format(dataframe, date_time):\n",
    "    \"\"\"\n",
    "    Function for post-processing prediction returned. Returns a dataframe of\n",
    "    prediction that contains the predictions for end of today up to tomorrow:\n",
    "        ie. from SP_47 TODAY -> SP_46 of TOMORROW inclusive\n",
    "    Args:\n",
    "        dataframe: pd.DataFrame containing local_datetime and NIVPredictions\n",
    "        date_time: datetime chosen\n",
    "\n",
    "    Returns:\n",
    "        truncated pd.DataFrame\n",
    "    \"\"\"\n",
    "    now = date_time\n",
    "    tmr = date_time + timedelta(days=1)\n",
    "    start_pred = dataframe[dataframe[\"local_datetime\"].dt.day == now.day].iloc[-2].name\n",
    "    end_pred = dataframe[dataframe[\"local_datetime\"].dt.day == tmr.day].iloc[-3].name\n",
    "    dataframe_ = dataframe.loc[start_pred:end_pred]  # use loc because we're using index name\n",
    "    if dataframe_[\"NIVPredictions\"].isna().sum():\n",
    "        raise Exception(\"NIVPredictions has NaN after post-processing. \"\n",
    "                        \"Consider raising the backshift_niv_by hyperparameter\"\n",
    "                        \"during training\")\n",
    "\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def split_file_ands_save_to_s3(result: pd.DataFrame, datetime_val: dt.datetime):\n",
    "    \"\"\"\n",
    "    Split dt to 3 file base on date then save them to s3 \n",
    "    \n",
    "    \"\"\"\n",
    "    t_today     = datetime_val\n",
    "    t_yesterday = t_today - timedelta(days=1)\n",
    "    t_tomorrow  = t_today + timedelta(days=1)\n",
    "    \n",
    "    prev_data = result[result['local_datetime'].dt.day == t_yesterday.day]\n",
    "    save_data(\n",
    "        df = prev_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = yesterday\n",
    "    )\n",
    "\n",
    "    now_data = result[result['local_datetime'].dt.day == t_today.day]\n",
    "    save_data(\n",
    "        df = now_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = today\n",
    "    )\n",
    "\n",
    "    next_data = result[result['local_datetime'].dt.day == t_tomorrow.day]\n",
    "    save_data(\n",
    "        df = next_data,\n",
    "        file_name = 'prediction',\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "        date_partition = tomorrow\n",
    "    )\n",
    "\n",
    "\n",
    "def update_actual_columns(\n",
    "    result_file : pd.DataFrame,\n",
    "    merged_file : pd.DataFrame\n",
    "):\n",
    "    columns_name = ['ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', \n",
    "                    'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "    # print (merged_file['marketIndexPrice(MID)'].to_list())\n",
    "    result_file[columns_name] = merged_file[columns_name]\n",
    "    return result_file\n",
    "\n",
    "    \n",
    "def get_lastest_value(dframe, column_name):\n",
    "    tmp = dframe[[\"local_datetime\", column_name]].dropna(how='any').iloc[-1:]  \n",
    "    val = tmp.iloc[0][column_name]\n",
    "    stime = tmp.iloc[0]['local_datetime']\n",
    "    return val, stime\n",
    "\n",
    "\n",
    "# ============================================= End of Helper Functions ============================================= #\n",
    "\n",
    "# ============================================= PreProcess Functions ============================================= #\n",
    "\n",
    "def select_best_features(dataframe):\n",
    "    \"\"\"Remove redundant/not useful columns from dataframe\"\"\"\n",
    "    interconnector_cols = [c for c in dataframe.columns if \"int\" in c and \"FUEL\" in c]\n",
    "    dataframe[\"Total_Int(FUELHH)\"] = dataframe[interconnector_cols].apply(sum, axis=1)\n",
    "    dataframe[\"Total_Fossil(FUELHH)\"] = (dataframe[\"coal(FUELHH)\"] +\n",
    "                                         dataframe[\"ocgt(FUELHH)\"] +\n",
    "                                         dataframe[\"ccgt(FUELHH)\"] +\n",
    "                                         dataframe[\"oil(FUELHH)\"])\n",
    "    dataframe[\"Total_Other(FUELHH)\"] = (dataframe[\"biomass(FUELHH)\"] +\n",
    "                                        dataframe[\"other(FUELHH)\"] +\n",
    "                                        dataframe[\"nuclear(FUELHH)\"])\n",
    "    dataframe[\"Total_Hydro(FUELHH)\"] = dataframe[\"npshyd(FUELHH)\"] + dataframe[\"ps(FUELHH)\"]\n",
    "    fuelhh_drop_cols = [c for c in dataframe.columns if \"(FUELHH\" in c and \"total\" not in c.lower()]\n",
    "    # elexon generation cols\n",
    "    ele_gen_drop_cols = ([\"Wind_Offshore_fcst(B1440)\", \"Wind_Onshore_fcst(B1440)\"] +\n",
    "                         [c for c in dataframe.columns if \"windforfuelhh\" in c.lower()] +  # WINDFORFUELHH\n",
    "                         [c for c in dataframe.columns if \"(B16\" in c] +  # B16xx columns\n",
    "                         [\"Total_Load_fcst(B0620)\", \"Total_Load(B0610)\"])  # + act_ele_gen_drop_cols\n",
    "    # catalyst wind cols\n",
    "    wind_pc_cols = [c for c in dataframe.columns if \"pc\" in c.lower()]  # the actual is very corr. with other winds\n",
    "    sn_wind_cols = [c for c in dataframe.columns if \"sn\" in c.lower()]\n",
    "    cat_wind_drop_cols = [c for c in dataframe.columns if \"(Wind_\" in c and \"wind_act(Wind_unrestricted)\" != c]\n",
    "    cat_wind_drop_cols += wind_pc_cols + sn_wind_cols\n",
    "    # drop columns with redundant information\n",
    "    cols_to_remove = [\n",
    "        \"niv_act(Balancing_NIV_fcst_3hr)\",  # can be used in post process\n",
    "        \"hist_fcst(Balancing_NIV_fcst_3hr)\",  # bad feature, not to be used even in post process\n",
    "        \"niv(Balancing_NIV)\",  # can be used in post process\n",
    "        \"indicativeNetImbalanceVolume(DERSYSDATA)\",\n",
    "        \"systemSellPrice(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentSellVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAdjustmentBuyVolume(DERSYSDATA)\",\n",
    "        \"non_bm_stor(Balancing_detailed)\",\n",
    "        \"DAI(MELIMBALNGC)\",\n",
    "        \"DAM(MELIMBALNGC)\",\n",
    "        \"TSDF(SYSDEM)\",\n",
    "        \"ITSDO(SYSDEM)\",\n",
    "        \"temperature(TEMP)\",\n",
    "        \"ImbalancePriceAmount(B1770)\",\n",
    "        \"marketIndexPrice(MID)\",\n",
    "        \"marketIndexVolume(MID)\",\n",
    "        \"DATF(FORDAYDEM)\",\n",
    "        \"DAID(FORDAYDEM)\",\n",
    "        \"DAIG(FORDAYDEM)\",\n",
    "        \"DANF(FORDAYDEM)\"\n",
    "    ]\n",
    "    # day ahead auction cols\n",
    "    daa_gwstep_cols = [c for c in dataframe.columns if \"daa_gwstep\" in c.lower()]\n",
    "    daa_windrisk_cols = [c for c in dataframe.columns if \"windrisk\" in c.lower()]  # daauction/range/wind_risk @catalyst\n",
    "    daa_xgas_cols = [c for c in dataframe.columns if \"daa_xgas\" in c.lower()]  # xgas @Catalyst too high corr with gas\n",
    "    daa_gas_cols = [c for c in dataframe.columns if \"daa_gas\" in c.lower()]  # @Catalyst\n",
    "    daa_drop_cols = [\n",
    "        \"price_act(DAA)\",\n",
    "        \"price_fcst(DAA)\",\n",
    "        \"price_weighted(DAA)\",\n",
    "        \"hist_fcst(DAA_1D_8AM)\",\n",
    "        \"hist_fcst(DAA_1D_2PM)\",\n",
    "    ]\n",
    "    daa_drop_cols += daa_gwstep_cols + daa_windrisk_cols + daa_xgas_cols + daa_gas_cols\n",
    "    # drop the columns listed and return\n",
    "    cols_to_remove += (  # act_ele_gen_drop_cols +\n",
    "            fuelhh_drop_cols +\n",
    "            ele_gen_drop_cols +\n",
    "            cat_wind_drop_cols +\n",
    "            daa_drop_cols\n",
    "    )\n",
    "    return dataframe.drop(cols_to_remove, axis=1)\n",
    "\n",
    "\n",
    "def prepare_date_features(dataframe):\n",
    "    \"\"\"transform date features into tabular format\"\"\"\n",
    "    # cyclic encode the datetime information; sine because the cycle should start from 0\n",
    "    dataframe[\"sp_sin\"] = np.sin(dataframe[\"SettlementPeriod\"] * (2 * np.pi / 48))\n",
    "    dataframe[\"month_sin\"] = np.sin(dataframe[\"local_datetime\"].dt.month * (2 * np.pi / 12))\n",
    "    dataframe[\"week_sin\"] = np.sin((dataframe[\"local_datetime\"].dt.weekday + 1) * (2 * np.pi / 7))\n",
    "    # drop unparsed date column; note that SettlementDate is kept for later groupbys\n",
    "#     dataframe.drop([\"local_datetime\",  # information is already encoded\n",
    "#         \"SettlementPeriod\"], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def interpolate_outliers(dataframe, cutoff=2.5):\n",
    "    \"\"\"Replaces the outlier value with the previous and next value's average using the column's z statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        for idx in range(len(col)):\n",
    "            if np.abs(col[idx] - col_mean) > z_cutoff:\n",
    "                try:\n",
    "                    dataframe.loc[idx, col_name] = np.mean([col[idx - 1], col[idx + 1]])\n",
    "                except IndexError:\n",
    "                    # this only happens at either end of the input data, so we leave the value as is\n",
    "                    # it will be processed in the clip_outliers function if the cutoff there is less than or\n",
    "                    # equal to the cutoff here\n",
    "                    pass\n",
    "    return dataframe\n",
    "\n",
    "def clip_outliers(dataframe, cutoff=2):\n",
    "    \"\"\"Clip outlier using z-statistic\"\"\"\n",
    "    float_cols = [c for c in dataframe.select_dtypes(\"float\").columns if \"sin\" not in c]\n",
    "    for col_name in float_cols:\n",
    "        col = dataframe[col_name].to_numpy()\n",
    "        col_mean, col_std = col.mean(), col.std()  # save the mean and std of the dataframe column\n",
    "        z_cutoff = cutoff * col_std\n",
    "        lower_bound = col_mean - z_cutoff\n",
    "        upper_bound = col_mean + z_cutoff\n",
    "        for idx in range(len(col)):\n",
    "            row = col[idx]  # save to variable to avoid re-accessing\n",
    "            if np.abs(row - col_mean) > z_cutoff:\n",
    "                dataframe.loc[idx, col_name] = np.clip(row, lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_ewm_features(dataframe, window=8, alpha=1 - np.log(2) / 4):\n",
    "    \"\"\"Computes the exponentially moving weighted average features\"\"\"\n",
    "    weights = list(reversed([(1 - alpha) ** n for n in range(window)]))\n",
    "    ewma = partial(np.average, weights=weights)\n",
    "    ewm_cols = [c for c in dataframe.columns if \"Imbalance\" not in c and  # exclude target variable\n",
    "                \"(\" in c and  # this is to exclude time features\n",
    "                \"FUELHH\" not in c and  # exclude FUELHH features\n",
    "                \"cash_out\" not in c]  # exclude cash_out(Balancing_detailed) feature\n",
    "    for c in ewm_cols:\n",
    "        # compute daily ewm, parametrized by alpha\n",
    "        dataframe[f\"ewm_mean_{c}\"] = dataframe[c].rolling(window).apply(ewma)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def compute_shifted_features(dataframe):\n",
    "    \"\"\"Computes the features that can be shifted\"\"\"\n",
    "    # compute  backshifted features\n",
    "    bshift_2sp_cols = [\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bshift_2sp_cols:\n",
    "        dataframe[f\"bshift_2sp_{c}\"] = dataframe[c].shift(-2)\n",
    "    dataframe[\"bshift_4sp_boas(Balancing_detailed)\"] = dataframe[\"boas(Balancing_detailed)\"].shift(-4)\n",
    "    # compute back-differenced features\n",
    "    bdiff_cols = [\n",
    "        \"Generation_fcst(B1430)\",\n",
    "        \"boas(Balancing_detailed)\",\n",
    "        \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "        \"totalSystemAcceptedBidVolume(DERSYSDATA)\"\n",
    "    ]\n",
    "    for c in bdiff_cols:\n",
    "        dataframe[f\"bdiff_1sp_{c}\"] = dataframe[c].diff(-1)\n",
    "    # compute forward shifted feature; other fshifted features based on other cols did not perform well\n",
    "    fshift_cols = [\n",
    "        \"boas(Balancing_detailed)\",\n",
    "    ]\n",
    "    for c in fshift_cols:\n",
    "        dataframe[f\"fshift_1hr_{c}\"] = dataframe[c].shift(2)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df : pd.DataFrame):\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    df = select_best_features(df)  # remove columns based on previous experiment results\n",
    "    df = df[['local_datetime', 'SettlementDate', 'SettlementPeriod',\n",
    "       'Generation_fcst(B1430)', 'Solar_fcst(B1440)',\n",
    "       'ImbalanceQuantity(MAW)(B1780)',\n",
    "       'totalSystemAcceptedOfferVolume(DERSYSDATA)',\n",
    "       'totalSystemAcceptedBidVolume(DERSYSDATA)', 'boas(Balancing_detailed)',\n",
    "       'fwrd_trades(Balancing_detailed)', 'imbalngc(Balancing_detailed)',\n",
    "       'cash_out(Balancing_detailed)', 'intraday(Balancing_detailed)',\n",
    "       'wind_act(Wind_unrestricted)', 'Total_Int(FUELHH)', 'Total_Fossil(FUELHH)',\n",
    "       'Total_Other(FUELHH)', 'Total_Hydro(FUELHH)']]  # Select these columns only\n",
    "    df = prepare_date_features(df)  # parse date into cyclic features\n",
    "    df = interpolate_outliers(df, cutoff=2.5)  # interpolate using z statistics (avg next/before obs)\n",
    "    df = clip_outliers(df, cutoff=2)  # clip outliers using z statistics\n",
    "    df = compute_ewm_features(df, window=4)  # compute exponentially weighted moving average features\n",
    "    df = compute_shifted_features(df)  # compute features based on shifting and differencing\n",
    "    \n",
    "    # drop some columns that are not performant or are no longer needed\n",
    "    df = df.drop([\"SettlementDate\",\n",
    "                  \"wind_act(Wind_unrestricted)\",\n",
    "                  \"totalSystemAcceptedOfferVolume(DERSYSDATA)\",\n",
    "                  \"totalSystemAcceptedBidVolume(DERSYSDATA)\",\n",
    "                  \"Generation_fcst(B1430)\",\n",
    "                  \"intraday(Balancing_detailed)\",\n",
    "                  \"Solar_fcst(B1440)\"], axis=1)\n",
    "    print(\"Time taken to preprocess features in seconds:\", time.perf_counter() - t0)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    # df = df.drop(\"ImbalanceQuantity(MAW)(B1780)\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================= End of PreProcess Functions ============================================= #\n",
    "\n",
    "# ============================================= Support Main Functions ============================================= #\n",
    "list_cols = ['local_datetime', 'SettlementPeriod' , 'niv_predicted_1sp', 'niv_predicted_2sp',\n",
    "             'niv_predicted_3sp', 'niv_predicted_4sp', 'niv_predicted_5sp', 'niv_predicted_6sp',\n",
    "             'niv_predicted_7sp', 'niv_predicted_8sp', 'dayahead_morning', 'dayahead_afternoon', \n",
    "             'ImbalanceQuantity(MAW)(B1780)', 'ImbalancePriceAmount(B1770)', 'marketIndexPrice(MID)', 'price_act(DAA)']\n",
    "\n",
    "\n",
    "def check_file_is_exists(bucket_name, write_path, path, day2write, cols=list_cols):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    start_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\")\n",
    "    end_date = dt.datetime.strptime(day2write, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    \n",
    "    prefix_path = path + f\"/year={start_date.year}/month={start_date.month}/day={start_date.day}/\"\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix_path))\n",
    "    if not len(file_list):\n",
    "        \n",
    "        print(\"No file in \", prefix_path)\n",
    "        # Create new Template\n",
    "        df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq=\"30T\"), columns=cols[1:])\n",
    "        df = df.head(48)\n",
    "        df.index.name = cols[0]\n",
    "        df['SettlementPeriod'] = range(0, len(df))\n",
    "        df['SettlementPeriod'] = df['SettlementPeriod'].apply(lambda x: x % 48 + 1)\n",
    "        df = df.reset_index()\n",
    "        print(f\"Creating empty file at \", day2write)\n",
    "        \n",
    "        save_data(df = df,\n",
    "                  file_name = 'prediction',\n",
    "                  path = os.path.join(write_path, path),\n",
    "                  date_partition = day2write\n",
    "        )\n",
    "        print(f\"Created empty file at \", day2write)\n",
    "    else: \n",
    "        print('File Exits')\n",
    "\n",
    "\n",
    "def override(df: pd.DataFrame, column_name, value, stime):\n",
    "    inx = df.index[(df['local_datetime'] > stime - timedelta(minutes=10))\n",
    "                   &(df['local_datetime'] < stime + timedelta(minutes=10))].values[0] \n",
    "    df.at[inx, column_name] = value\n",
    "    # print (f\"Lasted {column_name} Index in Result file : \", inx, ', at ', stime)\n",
    "    return df\n",
    "\n",
    "\n",
    "def day_ahead(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    \"\"\" \n",
    "    Morning prediction deadline: 8:45    you can run at 8:30 (and 7:30)\n",
    "    Afternoon prediction deadline: 14:30 you can run at 14:15 (and 13:15)\n",
    "    So I set it run at 8:10 (or 7:10), 14:10 (or 13:10) should meet the deadline\n",
    "    \"\"\"\n",
    "    if (datetime.now().hour in [7, 8, 13, 14]) & (datetime.now().minute < 15):\n",
    "        # split target out of payload input\n",
    "        dayahead_payload = processed_df.drop([\n",
    "            \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "            \"local_datetime\",\n",
    "            \"SettlementPeriod\",\n",
    "        ], axis=1).to_numpy()\n",
    "\n",
    "        half_day = 'morning' if datetime.now().hour < 12 else 'afternoon'\n",
    "        print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "\n",
    "        ##### invoke model endpoint for getting predictions #####\n",
    "        ### Get the newest model file aka \"TargetModel\"\n",
    "        list_files = []\n",
    "        for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "            if f\"lgbm-regressor-dayahead-{half_day}\" in object_summary.key:\n",
    "                list_files.append(object_summary.key)\n",
    "        assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "        target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "        print(\"Lastest Day-Ahead target Model :\", target_model)\n",
    "\n",
    "        ### Invoke DAY-AHEAD Endpoint & Get Result\n",
    "        runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "        print (\"Invoking day-ahead endpoint with Payload data\")\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=\"lgbm-regressor-endpoint\",\n",
    "            ContentType =\"application/JSON\",\n",
    "            TargetModel = target_model,\n",
    "            Body=json.dumps(dayahead_payload.tolist()),\n",
    "        )\n",
    "        dayahead_arr = json.loads(response[\"Body\"].read())\n",
    "        print(\"Success Invoking Endpoint!\")\n",
    "\n",
    "\n",
    "        # store the prediction in a new df so that we can do some post processing\n",
    "        # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "        tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "        tmp_result_df[\"NIVPredictions\"] = dayahead_arr\n",
    "        tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "        # make a dummy df so that we can shift the predictions\n",
    "        dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "        dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "            processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "            freq=\"30T\"\n",
    "        )\n",
    "        dummy_df[\"NIVPredictions\"] = np.nan\n",
    "        dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                             processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "        dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "        # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "        tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "        tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(96)\n",
    "        tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "        print(tmp_result_df.shape)\n",
    "        # truncate the predictions to the datetime range that we want, which is\n",
    "\n",
    "        tmp_result_df = truncate_to_dayahead_format(tmp_result_df, datetime.now())\n",
    "        dayahead_arr = tmp_result_df[\"NIVPredictions\"].tolist()                             \n",
    "\n",
    "        index = 48 + 46 ### SP = 47 so index = 46 & add 48 from whole yesterday\n",
    "        print('Length of dayahead array: ', len(dayahead_arr))\n",
    "        for i in range(len(dayahead_arr)):\n",
    "            result_files.at[index, 'dayahead_{}'.format(half_day)] = dayahead_arr[i]\n",
    "            index += 1\n",
    "\n",
    "        return result_files\n",
    "    else: \n",
    "        return result_files\n",
    "\n",
    "\n",
    "def intraday(\n",
    "    result_files : pd.DataFrame,\n",
    "    processed_df : pd.DataFrame,\n",
    "    \n",
    "):\n",
    "    # split target out of payload input\n",
    "    intraday_payload = processed_df.drop([\n",
    "        \"ImbalanceQuantity(MAW)(B1780)\",\n",
    "        \"local_datetime\",\n",
    "        \"SettlementPeriod\",\n",
    "    ], axis=1).to_numpy()\n",
    "    \n",
    "    print (\"length of preprocess_dataframe: \", processed_df.shape)\n",
    "    \n",
    "    ##### Invoke model endpoint for getting predictions #####\n",
    "    ### Get the newest model file aka \"TargetModel\"\n",
    "    list_files = []\n",
    "    for object_summary in model_bucket.objects.filter(Prefix=\"endpoint\"):\n",
    "        if f\"lgbm-regressor-intraday\" in object_summary.key:\n",
    "            list_files.append(object_summary.key)\n",
    "    assert len(list_files) == 1, \"check s3 should have one file\"\n",
    "    target_model = list_files[0].split(\"endpoint/\")[-1]\n",
    "    print(\"Lastest Intraday target Model :\", target_model)\n",
    "    \n",
    "    ### Invoke INTRADAY Endpoint & Get Result\n",
    "    runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "    print (\"Invoking endpoint with Intraday Payload\")\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=\"lgbm-regressor-endpoint\",\n",
    "        ContentType =\"application/JSON\",\n",
    "        TargetModel = target_model,\n",
    "        Body=json.dumps(intraday_payload.tolist()),\n",
    "    )\n",
    "    intraday_arr = json.loads(response[\"Body\"].read())\n",
    "    print(\"Success Invoking Intraday Endpoint!\")\n",
    "    \n",
    "    \n",
    "    # store the prediction in a new df so that we can do some post processing\n",
    "    # this dataframe only extends up to the last row NOT containing NaN, which is\n",
    "    tmp_result_df = processed_df.copy()[[\"local_datetime\", \"SettlementPeriod\"]]\n",
    "    tmp_result_df[\"NIVPredictions\"] = intraday_arr\n",
    "    tmp_result_df[\"local_datetime\"] = pd.to_datetime(tmp_result_df[\"local_datetime\"])\n",
    "\n",
    "    # make a dummy df so that we can shift the predictions\n",
    "    dummy_df = pd.DataFrame()  # .reset_index(drop=True)\n",
    "    dummy_df[\"local_datetime\"] = pd.date_range(\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(minutes=30),\n",
    "        processed_df[\"local_datetime\"].iloc[-1] + timedelta(days=2),  # timedelta 2 days to make sure it works\n",
    "        freq=\"30T\"\n",
    "    )\n",
    "    dummy_df[\"NIVPredictions\"] = np.nan\n",
    "    dummy_df[\"SettlementPeriod\"] = range(processed_df[\"SettlementPeriod\"].iloc[-1],\n",
    "                                         processed_df[\"SettlementPeriod\"].iloc[-1] + len(dummy_df))\n",
    "    dummy_df[\"SettlementPeriod\"] = dummy_df[\"SettlementPeriod\"] % 48 + 1\n",
    "\n",
    "    # concatenate to results dataframe and forward shift NIV predictions the same amount that was backshifted\n",
    "    tmp_result_df = pd.concat([tmp_result_df, dummy_df], axis=0).reset_index(drop=True)\n",
    "    tmp_result_df[\"NIVPredictions\"] = tmp_result_df[\"NIVPredictions\"].shift(48)\n",
    "    tmp_result_df = tmp_result_df.dropna(subset=[\"NIVPredictions\"])\n",
    "    print(\"Shape of Result: \", tmp_result_df.shape)\n",
    "    \n",
    "    #### Get Lastest NIV Time to identify where to fill the result\n",
    "    niv_val, niv_time = get_lastest_value(processed_df, \"ImbalanceQuantity(MAW)(B1780)\")\n",
    "    print(\"Lastest NIV : \", niv_val, niv_time)\n",
    "    \n",
    "    tmp_result_df = tmp_result_df[(tmp_result_df['local_datetime'] > niv_time) & \n",
    "                                  (tmp_result_df['local_datetime'] < niv_time + timedelta(hours = 4.5))]\n",
    "    print(tmp_result_df)\n",
    "    intraday_arr = tmp_result_df[\"NIVPredictions\"].tolist()   \n",
    "    print('Length of intraday array: ', len(intraday_arr))\n",
    "    \n",
    "    # index = result_files.index[result_files['SettlementTime'].dt.datetime == niv_time].values[0]\n",
    "    index = result_files.index[(result_files['local_datetime'] > niv_time + timedelta(minutes=20))\n",
    "                               &(result_files['local_datetime'] < niv_time + timedelta(minutes=40))].values[0]\n",
    "    \n",
    "    print ('Intraday index = ', index)\n",
    "    # Insert Value\n",
    "    for i in range(len(intraday_arr)):\n",
    "        index += 1\n",
    "        result_files.at[index, 'niv_predicted_{}sp'.format(i + 1)] = intraday_arr[i]\n",
    "        print(index, result_files.at[index, 'local_datetime'], intraday_arr[i], '\\n')\n",
    "\n",
    "    return result_files\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    merged_df = read_parquet_tables(\n",
    "        file_name=\"merged\",\n",
    "        start_date = prev_day,\n",
    "        end_date = tomorrow,\n",
    "        path = read_path,\n",
    "    )\n",
    "    print (\"length of Merged dataframe: \", merged_df.shape)\n",
    "    merged_df[\"local_datetime\"] = pd.to_datetime(merged_df[\"local_datetime\"])\n",
    "    df = preprocess_dataframe(merged_df)\n",
    "    \n",
    "    ### Check result exist: if not, create a new template\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, today, cols=list_cols)\n",
    "    check_file_is_exists(my_bucket, write_path, pred_folder, tomorrow, cols=list_cols)\n",
    "    \n",
    "    result_files = read_parquet_tables(\n",
    "        file_name= \"prediction\", \n",
    "        start_date = yesterday,\n",
    "        end_date = tomorrow,\n",
    "        path = os.path.join(write_path, pred_folder),\n",
    "    )\n",
    "    print(\"Shape of Result_file \", result_files.shape)\n",
    "    if 'SettlementTime' in result_files.columns:\n",
    "        result_files.rename(columns = {'SettlementTime':'local_datetime', 'SP':'SettlementPeriod'}, inplace = True) \n",
    "    \n",
    "    result_files['local_datetime'] = pd.to_datetime(result_files[\"local_datetime\"])\n",
    "    \n",
    "    result_files = day_ahead(result_files, df)\n",
    "    result_files = intraday (result_files, df)\n",
    "    \n",
    "    ### Update actual values in order to calculate PnL in the future\n",
    "    merged_file = merged_df[merged_df['local_datetime'] >= dt.datetime.strptime(yesterday, \"%Y-%m-%d\")]\n",
    "    print (\"# Row of actual data before joining to result files\", merged_file.shape[0])\n",
    "    result_files = update_actual_columns(result_files, merged_file.reset_index(drop = True))\n",
    "    \n",
    "    ##### Delivery files to S3 \n",
    "    split_file_ands_save_to_s3(result_files, dt.datetime.strptime(today, \"%Y-%m-%d\"))\n",
    "\n",
    "    print('Testing successfully!')\n",
    "\n",
    "################################# Initialize ########################################################\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = 'niv-predictions'   \n",
    "pred_folder = 'lgbm-prediction' \n",
    "bucket = s3.Bucket(my_bucket)\n",
    "model_bucket = s3.Bucket('lgbm-model-storage')\n",
    "resource = boto3.resource('s3')\n",
    "read_path = \"s3://scgc/data/merged\"\n",
    "write_path = \"s3://\" + my_bucket\n",
    "prev_day = (dt.datetime.now() - timedelta(days=4)).strftime(\"%Y-%m-%d\")\n",
    "yesterday= (dt.datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today    = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "tomorrow = (dt.datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "lambda_handler(\"\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
