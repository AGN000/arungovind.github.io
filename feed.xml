<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://zakariapatel.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zakariapatel.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-26T11:38:11-04:00</updated><id>https://zakariapatel.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Autoencoders Explained</title><link href="https://zakariapatel.github.io/blog/2023/giscus-comments/" rel="alternate" type="text/html" title="Autoencoders Explained"/><published>2023-05-05T11:59:00-04:00</published><updated>2023-05-05T11:59:00-04:00</updated><id>https://zakariapatel.github.io/blog/2023/giscus-comments</id><content type="html" xml:base="https://zakariapatel.github.io/blog/2023/giscus-comments/"><![CDATA[ <h2 id="the-standard-autoencoder">The Standard Autoencoder</h2> <p>An autoencoder is a special type of neural network architecture that learns to reconstruct its input. It consists of two components: an encoder to learn a representation of the input, and decoder to construct this representation back into the input. However, the actual architecture is a single feed-forward network, with a bottleneck layer marking the boundary between the two components.</p> <p>Once an input sample of dimension \(N\) passes through the encoder, we obtain a latent representation of it that has dimensionality \(D\). As autoencoders are tools for dimnesionality reduction, \(D \ll N\). Subsequently, the decoder takes this latent representation and produces an output of size \(N\), that resembles the input as closely as possible.</p> <p>If the decoder is able to reconstruct the input simply using the latent representation, the latter must contain all the most relevant information about the original input.</p> <p>From a mathematical perspective, the encoder can be represented as a function \(f\) that maps an input vector \(\mathbf{x}\) to a latent representation \(\mathbf{z}\):</p> \[\mathbf{z} = f(\mathbf{x})\] <p>The latent representation \(\mathbf{z}\) is \textit{typically} of lower dimensionality than the original input vector \(\mathbf{x}\).</p> <p>Once the input data has been compressed to the lower-dimensional representation $\mathbf{z}$, the decoder network takes this representation and maps it back to a reconstructed version $\mathbf{x’}$ of the original input:</p> \[\mathbf{x'} = g(\mathbf{z})\] <p>The encoder function \(f\) and decoder function \(g\) are implemented as a neural network with one or more hidden layers.</p> <p>The autoencoder’s training objective entails minimizing the difference between the original input vector \(\mathbf{x}\) and its reconstructed version \(\mathbf{x'}\). This can be accomplished by defining a loss function that measures the difference between \(\mathbf{x}\) and \(\mathbf{x'}\), and then optimizing the parameters of the encoder and decoder networks to minimize this loss function.</p> <p>The mean squared error (MSE) loss is well suited for this purpose. It measures the average squared difference between the original input vector and its reconstructed version:</p> \[\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i - \mathbf{x'}_i)^2\] <p>where \(n\) is the number of elements in the input vector.</p> <p>Another common loss function is the binary cross-entropy (BCE) loss, which is used when the input vector consists of binary values (e.g. 0s and 1s):</p> \[\mathcal{L}_{\text{BCE}} = -\frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i \log(\mathbf{x'}_i) + (1-\mathbf{x}_i) \log(1-\mathbf{x'}_i))\] <p>where \(\log\) represents the natural logarithm.</p> <h2 id="generation">Generation</h2> <p>Consider the space \(\mathbf{S}\), which contains all possible images of shape \(\mathbf{H \times W}\). Consider also the smaller subspace \(\mathbf{S_N}\) of natural images. If we want to generate an image in \(\mathbf{S_N}\), we have nowhere to start. What set of \(\mathbf{HW}\) pixels will place us in this space to begin with? One can begin by randomly sampling each pixel value, but it is incredibly unlikely that all \(\mathbf{HW}\) pixels (or even a small subset of them, for that matter) will result in anything sensible.</p> <p>To make the problem easier, suppose we have some way of picking a good initialization, such that we are already in \(\mathbf{S_N}\). How do we generate another image? In other words, how do you move from one point in the subspace to another? Once again, we can use the idea of applying random perturbations to each pixel, but this unlikely to result in another valid natural image. The result will be a corrupted version of the original.</p> <p>Clearly, \(\mathbf{S_N}\) is small compared to \(\mathbf{S}\), and it is very tricky to navigate. This makes it difficult to 1) find a good starting point for generation, and 2) traverse the space. Clearly, working in image space poses a few challenges. Is it possible to build another space that does not suffer from these issues?</p> <p>As we shall see, a special type of autoencoder, the \(\textbf{Variational Autoencoder}\) (VAE), does exactly this. I attempt to motivate it in a way that lends itself to building one’s intuition, but there are plenty of alternative explanations out there if this one does not make sense to you.</p> <h2 id="variational-autoecoders">Variational Autoecoders</h2> <p>Consider the simple graphical model \(\mathbf{Z} \rightarrow \mathbf{X}\), describing a generative process wherein a latent variable \(\mathbf{z}\) is converted into an item of interest \(\mathbf{x}\) (which we will treat as an image from this point onward). The variable \(\mathbf{z}\) exists in a latent space, from which we can map to a valid datapoint in image space, \mathbf{x}. This space should be constructed such that it circumvents the issues that we previously encountered.</p> <p>While \(\mathbf{x}\in \mathcal{R}^d\) is observed, it is a product of \(\mathbf{z}\in \mathcal{R}^k\), a latent space variable. We can’t observe \(\mathbf{z}\) (otherwise it wouldn’t be latent), but it may be possible to infer it after observing its generation, \(\mathbf{x}\). We’re interested in this latent variable \(\mathbf{z}\) because it maps to our observations \(\mathbf{x}\).</p> <p>In order to treat this as a generative process, we begin by defining a distribution \(p(\mathbf{z})\) from which we can randomly sample a latent vector. Subsequently, we could provide \(\mathbf{z}\) as input to a (for simplicity) deterministic decoder \(G_\theta(\mathbf{z})\), producing generated samples \(\mathbf{x'}\). However, the process of converting a latent representation into an image is complicated. As usual, such complicated functions can be approximated by a neural network, and so \(G_\theta\) is implemented as such.</p> <p>The issue with this approach occurs in how we formulate our objective. A natural way of training a model on this task may entail a maximum likelihood objective, where we try to optimize the parameters of the model to maximize the likelihood of the data, \(p(\mathbf{x})=\int p(\mathbf{z})p(\mathbf{x}\vert\mathbf{z}) d\mathbf{z}\) (using Bayes’ rule to reformulate our MLE objective actually provides some hints in how we might design our architecture). Here, \(p(\mathbf{z})\) is a prior distribution for \(\mathbf{z}\), which is often chosen to be a Gaussian. If we had access to both \(p(\mathbf{z})\) and \(p(\mathbf{x}\vert\mathbf{z})\) which worked together (perhaps through joint training), we could sample \(\mathbf{z}\) from the former, and subsequently use it to sample \(\mathbf{x}\) from the latter distribution. If we assume that this scheme works, and we have both distributions in hand, then we have solved both issues that arose earlier. Since we know the distribution \(p(\mathbf{z})\) (we explicitly chose it to be Gaussian with some pre-selected mean and variance), we have some reference point to begin with. For instance, if we somehow succeeded in getting \(p(\mathbf{z})\) and \(p(\mathbf{x}\vert\mathbf{z})\) working together, and \(p(\mathbf{z})\) was a Gaussian with \(\mathbf{\mu}=\mathbf{0}\), then we could just start sampling from there. The second issue might also be resolved, as \(p(\mathbf{x}\vert\mathbf{z})\) is a continuous distribution, and if \(p(\mathbf{x}\vert\mathbf{z})\) is able to generate samples from it, then we must have encoded most of the relevant information into \(p(\mathbf{z})\). The choice of \(p(\mathbf{z})\) being Gaussian may seem rather ad-hoc, but Gaussians have much to offer in the way of computational convenience.</p> <p>The second term, \(p(\mathbf{x}\vert \mathbf{z})\), represents the probability distribution over the decoder’s outputs, conditioned on the input \(\mathbf{z}\). If the decoder is deterministic, each input \(\mathbf{z}\) corresponds to a single output \(\mathbf{x'}\), meaning the distribution \(p(\mathbf{x}\vert \mathbf{z})\) is zero everywhere except where single point where \(\mathbf{x} = \mathbf{x'}\). As such, there is often no training signal. The problem is made worse when \(k\ll d\). The decoder must map the low dimensional latent code \(\mathbf{z}\) to a much higher dimension.</p> <p>We deal with this by introducing a \textit{noisy channel model},</p> \[p_\theta(\mathbf{x}\vert\mathbf{z}) = \mathcal{N}(\mathbf{x}; G_\theta(\mathbf{z}), \eta \mathbf{I}).\] <p>This formulation ensures that the conditional probability of \(\mathbf{x}\) is non-zero everywhere. Again, we’ve assumed a Gaussian for computational convenience, with mean \(G_\theta(\mathbf{z})\) and covariance \(\eta\). Each latent code now corresponds to a probability distribution over \(\mathbf{x}\), ensuring that we obtain a non-zero training signal. Computing \(p(\mathbf{x})=\int p(\mathbf{z}) p(\mathbf{x}\vert\mathbf{z}) d\mathbf{z}\) requires integration over each dimension of \(\mathbf{z}\).</p> <p>Another issue with the maximum likelihood objective is that it is possibly intractable. We chose to approximate the likelihood function \(p(\mathbf{x}\vert\mathbf{z})\) with a neural network \(G_\theta\). Obviously, \(G_\theta\) is a very complicated function - there is no hope of analytically integrating it.</p> <p>Numerical methods aren’t necessarily sufficient here, either. Computing \(p(\mathbf{x})\) involves integration over all latent variables. As such, we must consider all possible combinations of values that each latent variable can take, leading to a complexity that scales with the number of latent variables. In short, a high dimensional latent space leads to a computationally burdensome integral. So, we have reached the conclusion that computing \(p(\mathbf{x})\) is clearly intractable.</p> <p>We will briefly digress to reiterate how we planned to use this model in a generative capacity. We must be able to \textit{randomly} sample \(\mathbf{z}\), and we can subsequently pass it into the decoder \(G_\theta\) to produce a sample \(\mathbf{x'}\). This is certainly a plausible scheme if we had some way of sampling $\mathbf{z}$. In fact, we do. We have already chosen \(p(\mathbf{z})\) to be a Gaussian with mean \(\mathbf{\mu} = \mathbf{0}\) and unit covariance. If \(G_\theta\) has been trained to take latent vectors from this prior distribution and decode them into samples \(\mathbf{x'}\), then we have a generative model.</p> <p>How does the decoder actually learn to map latent vectors to samples? We could ask the decoder to produce samples \(\mathbf{x'}\) that are similar to some ground truth \(\mathbf{x}\) based on a sampled latent vector \(\mathbf{z}\). However, this choice of latent vector is arbitrary, as we have chosen it ourselves. We would rather let the network learn a latent space on its own, as the network can learn something which has a meaningful structure. To this end, we introduce an encoder network \(q_\phi(\mathbf{z}\vert\mathbf{x})\) which approximates the posterior distribution \(p(\mathbf{z}\vert\mathbf{x})\)</p> <p>Just like the standard autoencoder, this encoder produces a latent vector \(\mathbf{z}\) that the decoder must try to reconstruct into \(\mathbf{x}\). Then, training the encoder and decoder together produces a meaningful latent space.</p> <h2 id="variational-inference">Variational Inference</h2> <p>The approximation \(q_\phi(\mathbf{z}\vert\mathbf{x})\) should be as close to \(p(\mathbf{z}\vert\mathbf{x})\) as possible. One way to determine how different two distributions are is to measure their Kullback-Leibler (KL) divergence (formally, it is not a distance metric as it is not symmetric, i.e. \(D_{KL}(p\vert\vert q) \neq D_{KL}(q\vert\vert p)\)). For a discrete distribution,</p> \[KL(p \vert\vert q) = \sum_{c=1}^{M}p_c \log{\frac{q_c}{p_c}}\] <p>In our case, we convert the sum to an integral because the approximate and true posteriors are continuous. Their KL divergence can be written as follows,</p> \[\begin{alignat}{1} D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x}))\\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{z}\vert\mathbf{x})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z}\\= \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{x})q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z}\\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\left[ \log\left(\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) - \log\left(p(\mathbf{x})\right) \right]d\mathbf{z} \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(p(\mathbf{x})\right) d\mathbf{z} \end{alignat}\] <p>\(p(\mathbf{x})\) does not depend on \(\mathbf{z}\), and the integral of \(q(\mathbf{z}\vert\mathbf{x})\) over all space must be equal to 1,</p> \[\begin{alignat}{1} = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - \log\left(p(\mathbf{x})\right) \int q_\phi(\mathbf{z}\vert\mathbf{x}) d\mathbf{z}\\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - \log\left(p(\mathbf{x})\right) \end{alignat}\] <p>Rearranging this expression,</p> \[\begin{alignat}{1} D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{z}\vert\mathbf{x})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - \log\left(p(\mathbf{x})\right) \log\left(p(\mathbf{x})\right) \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) d\mathbf{z} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\left[\log\left(\frac{p(\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right) - \log\left( p(\mathbf{x}\vert\mathbf{z})\right) \right] d\mathbf{z} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \\ = \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left(\frac{p(\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right)d\mathbf{z} - \int q_\phi(\mathbf{z}\vert\mathbf{x})\log\left( p(\mathbf{x}\vert\mathbf{z})\right) d\mathbf{z} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{x}\vert\mathbf{z}\vert\mathbf{x})) \end{alignat}\] <p>We recognize the first term as the KL divergence between \(q_\phi(\mathbf{z}\vert\mathbf{x})\) and \(p(\mathbf{z})\), and the second as the expectation of \(p(\mathbf{x}\vert\mathbf{z})\) with respect to the approximate posterior distribution \(q_\phi(\mathbf{z}\vert\mathbf{x})\).</p> \[\begin{alignat}{1} \log\left(p(\mathbf{x})\right) = \left[ D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z})) - \mathbb{E}_ {q(\mathbf{z}\vert\mathbf{x})}\left[\log p(\mathbf{z})\right] \right] - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \\ = \text{ELBO} - D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x})) \end{alignat}\] <p>The observed data does not change - \(\log(p(\mathbf{x}))\) must be a constant. However, the terms which sum up to the log data likelihood do vary individually. If we increase one of the terms, we must decrease the others. Our objective was to minimize \(D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x}))\), but obtaining \(p(\mathbf{z}\vert\mathbf{x})\) is difficult (which is why we are approximating it with \(q_\phi(\mathbf{z}\vert\mathbf{x})\) in the first place). Instead, we can maximize the we named “ELBO”, which \textit{is} tractable. This term is the \textbf{variational lower bound}. Given our above reasoning, maximizing the ELBO will minimize \(D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\vert\vert p(\mathbf{z}\vert\mathbf{x}))\), and we will obtain an encoder which closely approximates the true posterior distribution.</p> ]]></content><author><name>Zakaria Patel</name></author><summary type="html"><![CDATA[Exploring generative models...]]></summary></entry></feed>